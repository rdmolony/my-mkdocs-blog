{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to my blog \u00b6 I'm a software engineer at Mainstream Renewable Power While attempting to modernise legacy Django codebase I've written down a few learnings in today-I-learned ( til ) format that perhaps others might find useful... Inspired by simonw/til","title":"Welcome to my blog"},{"location":"#welcome-to-my-blog","text":"I'm a software engineer at Mainstream Renewable Power While attempting to modernise legacy Django codebase I've written down a few learnings in today-I-learned ( til ) format that perhaps others might find useful... Inspired by simonw/til","title":"Welcome to my blog"},{"location":"til/add-a-custom-column-to-django-tables2/","text":"I can add a custom column for which no corresponding database field exists to django-tables2 rendered HTML table once I add a corresponding method to my model. I can target this method via accessor ... # models.py class MyModel(models.Model): my_field = models.TextField() my_field_2 = models.IntegerField() def my_function(self): # Return some calculated value based on the entry return my_value # tables.py class MyTable(tables.Table): my_extra_column = tables.Column( accessor='my_function', verbose_name='My calculated value' ) class Meta: fields = ['my_field', 'my_field_2', 'my_extra_column'] model = MyModel django - Custom columns in django_tables2 - Stack Overflow ... or I can add it as a property ... # models.py class Person(models.Model): first_name = models.CharField(max_length=200) family_name = models.CharField(max_length=200) @property def name(self): return \"{} {}\".format(self.first_name, self.family_name) # tables.py class PersonTable(tables.Table): name = tables.Column() https://django-tables2.readthedocs.io/en/latest/pages/ordering.html django \u00b6 django-tables2 \u00b6 python \u00b6","title":"Add a custom column to django tables2"},{"location":"til/add-a-custom-column-to-django-tables2/#django","text":"","title":"django"},{"location":"til/add-a-custom-column-to-django-tables2/#django-tables2","text":"","title":"django-tables2"},{"location":"til/add-a-custom-column-to-django-tables2/#python","text":"","title":"python"},{"location":"til/add-autocompletion-to-django-filters/","text":"I use filters to enable users to query a table of sensors by country, project & station name. As there are a lot of values, I'm using CharField with icontains to let users type CH and filter out Chile . I want to add autocompletion so that Chile appears in a dropdown list on typing CH I can use django-autocomplete-light alongside django-filter to do so. I must first add a new autocompletion view ... from dal import autocomplete class CountryAutocomplete(autocomplete.Select2QuerySetView): def get_queryset(self): if self.q: qs = FkCountry.objects.filter(name__istartswith=self.q) else: qs = FkCountry.objects.objects.all() return qs from views import CountryAutocomplete urlpatterns = [ ... url( \"^country-autocomplete$\", CountryAutocomplete.as_view(), name=\"country-autocomplete\" ), ] Note : this is queried by django-autocomplete-light Javascript to generate the autocompletion values I can override the default django-filter widget for the ModelChoiceFilter ... from dal import autocomplete import django_filter as filters from stationmanager import models class SensorQualityFilter(filters.FilterSet): class Meta: model = SensorQuality fields = [] country = filters.ModelChoiceFilter( field_name=\"station__idproject__idcountry__name\", label=\"Country\", queryset=models.FkCountry.objects.all(), widget=autocomplete.ModelSelect2('country-autocomplete'), ) ... ... and link it to my table view ... from django_filters.views import FilterView import django_tables2 as tables from stationmanager import models class StationListView(tables.SingleTableMixin, FilterView): model = models.Station table_class = tables.StationTable template_name = 'stationmanager/station_list.html' filterset_class = filters.StationFilter ... and add some Javascript to my template footer ... {% block head %} {% load static %} {% load render_table from django_tables2 %} {% load export_url from django_tables2 %} {% load bootstrap3 %} {% endblock %} {% block body %} <div class=\"container\"> {% if filter %} <form action=\"\" method=\"get\" class=\"form form-inline\"> {% bootstrap_form filter.form layout='inline' %} <div class=\"text-right\"> <button type=\"submit\" id=\"filter\" class=\"btn\">Filter</button> </div> </form> {% endif %} </div> {% render_table table 'django_tables2/bootstrap.html' %} {% endblock %} {% block footer %} {{ filter.form.media }} {% endblock %} https://github.com/carltongibson/django-filter/issues/211 https://stackoverflow.com/questions/40502794/django-filter-with-django-autocomplete-light https://stackoverflow.com/questions/38799632/django-filter-with-django-autocomplete-light django \u00b6 django-tables2 \u00b6 django-filter \u00b6 django-autocomplete-light \u00b6","title":"Add autocompletion to django filters"},{"location":"til/add-autocompletion-to-django-filters/#django","text":"","title":"django"},{"location":"til/add-autocompletion-to-django-filters/#django-tables2","text":"","title":"django-tables2"},{"location":"til/add-autocompletion-to-django-filters/#django-filter","text":"","title":"django-filter"},{"location":"til/add-autocompletion-to-django-filters/#django-autocomplete-light","text":"","title":"django-autocomplete-light"},{"location":"til/add-conda-to-the-powershell-profile/","text":"Add conda to the powershell profile \u00b6 I wanted to automatically activate conda in powershell on startup. In zsh I can edit the ~/.zshrc , in powershell the equivalent is the unwieldy C:\\Users\\Rowan.Molony\\OneDrive - Mainstream Renewable Power\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1 or $PROFILE . conda supports editing this profile by running conda init powershell in the Anaconda powershell prompt that comes installed with miniconda or Anaconda . By default powershell does not allow the user to run unsigned scripts! To override this I had to change the execution policy to RemoteSigned for the CurrentUser as I don't have admin rights beyond this. Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser From the microsoft docs RemoteSigned means: Requires a digital signature from a trusted publisher on scripts and configuration files that are downloaded from the internet which includes email and instant messaging programs. Doesn't require digital signatures on scripts that are written on the local computer and not downloaded from the internet. Runs scripts that are downloaded from the internet and not signed, if the scripts are unblocked, such as by using the Unblock-File cmdlet. Risks running unsigned scripts from sources other than the internet and signed scripts that could be malicious. Now powershell boots with conda initialised and visible in the prompt! This means that the powershell shell in vscode now has access to conda . It also means that I can now style my prompt using oh-my-posh (see Styling powershell with oh-my-posh ) and make it permanent. :exclamation: See profile here powershell \u00b6","title":"Add conda to the powershell profile"},{"location":"til/add-conda-to-the-powershell-profile/#add-conda-to-the-powershell-profile","text":"I wanted to automatically activate conda in powershell on startup. In zsh I can edit the ~/.zshrc , in powershell the equivalent is the unwieldy C:\\Users\\Rowan.Molony\\OneDrive - Mainstream Renewable Power\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1 or $PROFILE . conda supports editing this profile by running conda init powershell in the Anaconda powershell prompt that comes installed with miniconda or Anaconda . By default powershell does not allow the user to run unsigned scripts! To override this I had to change the execution policy to RemoteSigned for the CurrentUser as I don't have admin rights beyond this. Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser From the microsoft docs RemoteSigned means: Requires a digital signature from a trusted publisher on scripts and configuration files that are downloaded from the internet which includes email and instant messaging programs. Doesn't require digital signatures on scripts that are written on the local computer and not downloaded from the internet. Runs scripts that are downloaded from the internet and not signed, if the scripts are unblocked, such as by using the Unblock-File cmdlet. Risks running unsigned scripts from sources other than the internet and signed scripts that could be malicious. Now powershell boots with conda initialised and visible in the prompt! This means that the powershell shell in vscode now has access to conda . It also means that I can now style my prompt using oh-my-posh (see Styling powershell with oh-my-posh ) and make it permanent. :exclamation: See profile here","title":"Add conda to the powershell profile"},{"location":"til/add-conda-to-the-powershell-profile/#powershell","text":"","title":"powershell"},{"location":"til/add-hoverable-info-to-django-tables2-column-names/","text":"I want to reduce the size of my columns by adding a hoverable info box beside the column name to provide more information. I can pass a string rendered by Django builtin render_html to verbose_name ! class StationTable(tables.Table): class Meta: model = Station template_name = \"django_tables2/bootstrap.html\" fields = (\"name\",) ... lag_days = tables.Column( format_html( 'Lag [Days]<span title=\"Most Recent Timestamp in Database\">\u2754</span>', ), ) python \u00b6 django \u00b6 django-tables2 \u00b6","title":"Add hoverable info to django tables2 column names"},{"location":"til/add-hoverable-info-to-django-tables2-column-names/#python","text":"","title":"python"},{"location":"til/add-hoverable-info-to-django-tables2-column-names/#django","text":"","title":"django"},{"location":"til/add-hoverable-info-to-django-tables2-column-names/#django-tables2","text":"","title":"django-tables2"},{"location":"til/backup-files-from-wsl-to-onedrive/","text":"Backup files from wsl to onedrive \u00b6 I want to backup source code saved in a Windows Subsystems for Linux (WSL) subfolder to OneDrive daily to add an extra degree of redundancy on top of GitHub . I have a local OneDrive folder that syncs automatically and Task Scheduler can run bat files at a given time. I created a simple file backup-stationmanager.bat ... xcopy ^ \"\\\\wsl$\\Ubuntu\\home\\rdmolony\\Code\\StationManager\\.git\" ^ \"C:\\Users\\Rowan.Molony\\OneDrive - Mainstream Renewable Power\\Documents\\Backups\\StationManager\\.git\" ^ /y /s ... that uses ^ instead of \\ for multiline, \\y to accept all and \\s to copy subfolders windows \u00b6","title":"Backup files from wsl to onedrive"},{"location":"til/backup-files-from-wsl-to-onedrive/#backup-files-from-wsl-to-onedrive","text":"I want to backup source code saved in a Windows Subsystems for Linux (WSL) subfolder to OneDrive daily to add an extra degree of redundancy on top of GitHub . I have a local OneDrive folder that syncs automatically and Task Scheduler can run bat files at a given time. I created a simple file backup-stationmanager.bat ... xcopy ^ \"\\\\wsl$\\Ubuntu\\home\\rdmolony\\Code\\StationManager\\.git\" ^ \"C:\\Users\\Rowan.Molony\\OneDrive - Mainstream Renewable Power\\Documents\\Backups\\StationManager\\.git\" ^ /y /s ... that uses ^ instead of \\ for multiline, \\y to accept all and \\s to copy subfolders","title":"Backup files from wsl to onedrive"},{"location":"til/backup-files-from-wsl-to-onedrive/#windows","text":"","title":"windows"},{"location":"til/block-duplicate-records-being-saved-in-django/","text":"Block duplicate records being saved in django \u00b6 I want to save unique flags indicating faulty time-series data to a database. I want Django to raise an error if an attempt is made to save a record that already exists. I could write some logic to catch this prior to writing the data each time but I'd rather have a validator at the model level. I thought that setting the unique=True constraint on each DateTimeField would prevent this. It didn't. It seems this constraint is intended to prevent duplicates across an entire field rather than across fields - so say \"Ireland\" can't be written twice to a field called \"countries\". I can instead validate by checking new values against existing values in the database prior to saving: class Flag(models.Model): id = models.AutoField(primary_key=True, db_column='id', blank=False, null=False) flag = models.CharField(db_column='Notes', null=False, blank=False, max_length=255) start_timestamp = models.DateTimeField(db_column='Start_Timestamp', blank=False, null=False) stop_timestamp = models.DateTimeField(db_column='Stop_Timestamp', blank=False, null=False) def is_a_new_record(self) -> bool: record_exists = Flag.objects.filter( flag=self.flag, start_timestamp=self.start_timestamp, stop_timestamp=self.stop_timestamp ).exists() return not record_exists def save(self, *args, **kwargs): if self.is_a_new_record(): return super().save(*args, **kwargs) else: print(f\"{self} is not a new record!\") django \u00b6","title":"Block duplicate records being saved in django"},{"location":"til/block-duplicate-records-being-saved-in-django/#block-duplicate-records-being-saved-in-django","text":"I want to save unique flags indicating faulty time-series data to a database. I want Django to raise an error if an attempt is made to save a record that already exists. I could write some logic to catch this prior to writing the data each time but I'd rather have a validator at the model level. I thought that setting the unique=True constraint on each DateTimeField would prevent this. It didn't. It seems this constraint is intended to prevent duplicates across an entire field rather than across fields - so say \"Ireland\" can't be written twice to a field called \"countries\". I can instead validate by checking new values against existing values in the database prior to saving: class Flag(models.Model): id = models.AutoField(primary_key=True, db_column='id', blank=False, null=False) flag = models.CharField(db_column='Notes', null=False, blank=False, max_length=255) start_timestamp = models.DateTimeField(db_column='Start_Timestamp', blank=False, null=False) stop_timestamp = models.DateTimeField(db_column='Stop_Timestamp', blank=False, null=False) def is_a_new_record(self) -> bool: record_exists = Flag.objects.filter( flag=self.flag, start_timestamp=self.start_timestamp, stop_timestamp=self.stop_timestamp ).exists() return not record_exists def save(self, *args, **kwargs): if self.is_a_new_record(): return super().save(*args, **kwargs) else: print(f\"{self} is not a new record!\")","title":"Block duplicate records being saved in django"},{"location":"til/block-duplicate-records-being-saved-in-django/#django","text":"","title":"django"},{"location":"til/cache-docker-compose-images-via-github-actions/","text":"TLDR As of 2022-06-23 both docker/bake-action or satackey/action-docker-layer-caching are good options. I had to hack around a little to use access the cached images in docker compose across multiple GitHub Actions steps. See the relevant sections below for more information. Context \u00b6 I wanted to setup GitHub Actions so that our test suite runs on all pull request or pushes to the main branch so that any changes don't break existing functionality. Specifically, we use docker compose at mainstreamrp-dev to setup a development environment for our Django web app. Our integration tests interact with Selenium (browser automation), Microsoft SQL Server & MySQL (databases) via docker compose . To set it up Docker reads a Dockerfile which specifies the system dependencies required to run the web app and stores the result in an image; we use python-poetry for Python and apt for Ubuntu . If any of these dependencies change we need to rebuild the image which can take several minutes. If they don't change between test runs I'd rather not wait several minutes every time and use caching to access a prebuilt image. As of 2022-06-23 setting up this caching was tricky. I created rdmolony/cache-docker-compose-images to experiment with 3rd party packages to do this for me: - docker/build-push-action: GitHub Action to build and push Docker images with Buildx - not recommended for docker compose - docker/bake-action: GitHub Action to use Docker Buildx Bake as a high-level build command - works - satackey/action-docker-layer-caching: \ud83d\udc33 Enable Docker layer caching in GitHub Actions - works Using these packages I was able to cache in one step, but I found it hard to access this cached image via docker compose in subsequent steps. satackey/action-docker-layer-caching \u00b6 The satackey/action-docker-layer-caching has good documentation. On every new pull request or push the image is built from scratch & on subsequent runs of GitHub Actions the cache is used instead. The only tricky thing was to invalidate the cache if the Dockerfile (or a poetry.lock file) changes so that we rebuild the image. After a little hunting, I found I could use the builtin function hashFiles to create a unique hash based on the Dockerfile which I could pass via the key & restore-keys parameters to satackey/action-docker-layer-caching to invalidate the cache if this file changes. # This is a basic workflow to help you get started with Actions name: CI # Controls when the workflow will run on: # Triggers the workflow on push or pull request events but only for the \"main\" branch push: branches: [ \"master\" ] pull_request: branches: [ \"master\" ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch: env: IMAGE_NAME: cache-docker-compose-images # A workflow run is made up of one or more jobs that can run sequentially or in parallel jobs: # This workflow contains a single job called \"build\" build: # The type of runner that the job will run on runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v3 - name: Login to Docker Hub uses: docker/login-action@v2 with: username: ${{ secrets.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} # Pull the latest image to build, and avoid caching pull-only images. # (docker pull is faster than caching in most cases.) - run: docker-compose pull # In this step, this action saves a list of existing images, # the cache is created without them in the post run. # It also restores the cache if it exists. - uses: satackey/action-docker-layer-caching@v0.0.11 # Ignore the failure of a step and avoid terminating the job. continue-on-error: true with: key: ${{ runner.os }}-${{ github.workflow }}-${{ hashFiles('**/Dockerfile') }}-{hash} restore-keys: | ${{ runner.os }}-${{ github.workflow }}-${{ hashFiles('**/Dockerfile') }} - name: Test run: docker compose run web - name: Tag image run: docker tag $IMAGE_NAME:latest ${{ secrets.DOCKERHUB_USERNAME }}/$IMAGE_NAME:$GITHUB_SHA - name: Push image to Docker Hub run: docker push ${{ secrets.DOCKERHUB_USERNAME }}/$IMAGE_NAME:$GITHUB_SHA docker/bake-action \u00b6 With the help of @crazy-max I worked out how to get docker compose run to play nicely with docker/bake-action See How to access the bake-action cached image in subsequent steps? \u00b7 Issue #81 \u00b7 docker/bake-action (github.com) I needed to: - Load environment variables in a .env in bash in a step as this wasn't yet supported in docker/bake-action - Specify load: true for the bake-action to save the Docker image so that the runner can access it in subsequent steps - Set the image name in docker-compose.yml to the same name as the bake-action ci.yml : # This is a basic workflow to help you get started with Actions name: CI # Controls when the workflow will run on: # Triggers the workflow on push or pull request events but only for the \"main\" branch push: branches: [ \"master\" ] pull_request: branches: [ \"master\" ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch: env: IMAGE_NAME: rdmolony/web PUSH_TAG: ${{ github.sha }} # A workflow run is made up of one or more jobs that can run sequentially or in parallel jobs: # This workflow contains a single job called \"build\" build: # The type of runner that the job will run on runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v3 # bake-action does not yet load .env - name: Export .env to GITHUB_ENV run: cat .env >> $GITHUB_ENV - name: Set up Docker Buildx uses: docker/setup-buildx-action@v2 - name: Build uses: docker/bake-action@v2.0.0 with: push: false load: true set: | web.cache-from=type=gha web.cache-to=type=gha web.tags=${{ env.IMAGE_NAME }} - name: Test run: docker compose run web docker-compose.yml : version: \"3.7\" services: web: build: context: . image: rdmolony/web command: echo \"Test run successful!\" Note : .env must be loaded into GITHUB_ENV to use the variables across steps Environment variables - GitHub Docs This action caches between workflow runs! This means that I won't have to rebuild if the image hasn't changed on the first pull request or push. I couldn't figure this out easily using satackey/action-docker-layer-caching though I imagine it's possible provided the right hash key is used. See Cache via bake action reuse by rdmolony \u00b7 Pull Request #6 \u00b7 rdmolony/cache-docker-compose-images (github.com) github-actions \u00b6 docker \u00b6 docker-compose \u00b6","title":"Cache docker compose images via github actions"},{"location":"til/cache-docker-compose-images-via-github-actions/#context","text":"I wanted to setup GitHub Actions so that our test suite runs on all pull request or pushes to the main branch so that any changes don't break existing functionality. Specifically, we use docker compose at mainstreamrp-dev to setup a development environment for our Django web app. Our integration tests interact with Selenium (browser automation), Microsoft SQL Server & MySQL (databases) via docker compose . To set it up Docker reads a Dockerfile which specifies the system dependencies required to run the web app and stores the result in an image; we use python-poetry for Python and apt for Ubuntu . If any of these dependencies change we need to rebuild the image which can take several minutes. If they don't change between test runs I'd rather not wait several minutes every time and use caching to access a prebuilt image. As of 2022-06-23 setting up this caching was tricky. I created rdmolony/cache-docker-compose-images to experiment with 3rd party packages to do this for me: - docker/build-push-action: GitHub Action to build and push Docker images with Buildx - not recommended for docker compose - docker/bake-action: GitHub Action to use Docker Buildx Bake as a high-level build command - works - satackey/action-docker-layer-caching: \ud83d\udc33 Enable Docker layer caching in GitHub Actions - works Using these packages I was able to cache in one step, but I found it hard to access this cached image via docker compose in subsequent steps.","title":"Context"},{"location":"til/cache-docker-compose-images-via-github-actions/#satackeyaction-docker-layer-caching","text":"The satackey/action-docker-layer-caching has good documentation. On every new pull request or push the image is built from scratch & on subsequent runs of GitHub Actions the cache is used instead. The only tricky thing was to invalidate the cache if the Dockerfile (or a poetry.lock file) changes so that we rebuild the image. After a little hunting, I found I could use the builtin function hashFiles to create a unique hash based on the Dockerfile which I could pass via the key & restore-keys parameters to satackey/action-docker-layer-caching to invalidate the cache if this file changes. # This is a basic workflow to help you get started with Actions name: CI # Controls when the workflow will run on: # Triggers the workflow on push or pull request events but only for the \"main\" branch push: branches: [ \"master\" ] pull_request: branches: [ \"master\" ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch: env: IMAGE_NAME: cache-docker-compose-images # A workflow run is made up of one or more jobs that can run sequentially or in parallel jobs: # This workflow contains a single job called \"build\" build: # The type of runner that the job will run on runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v3 - name: Login to Docker Hub uses: docker/login-action@v2 with: username: ${{ secrets.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} # Pull the latest image to build, and avoid caching pull-only images. # (docker pull is faster than caching in most cases.) - run: docker-compose pull # In this step, this action saves a list of existing images, # the cache is created without them in the post run. # It also restores the cache if it exists. - uses: satackey/action-docker-layer-caching@v0.0.11 # Ignore the failure of a step and avoid terminating the job. continue-on-error: true with: key: ${{ runner.os }}-${{ github.workflow }}-${{ hashFiles('**/Dockerfile') }}-{hash} restore-keys: | ${{ runner.os }}-${{ github.workflow }}-${{ hashFiles('**/Dockerfile') }} - name: Test run: docker compose run web - name: Tag image run: docker tag $IMAGE_NAME:latest ${{ secrets.DOCKERHUB_USERNAME }}/$IMAGE_NAME:$GITHUB_SHA - name: Push image to Docker Hub run: docker push ${{ secrets.DOCKERHUB_USERNAME }}/$IMAGE_NAME:$GITHUB_SHA","title":"satackey/action-docker-layer-caching"},{"location":"til/cache-docker-compose-images-via-github-actions/#dockerbake-action","text":"With the help of @crazy-max I worked out how to get docker compose run to play nicely with docker/bake-action See How to access the bake-action cached image in subsequent steps? \u00b7 Issue #81 \u00b7 docker/bake-action (github.com) I needed to: - Load environment variables in a .env in bash in a step as this wasn't yet supported in docker/bake-action - Specify load: true for the bake-action to save the Docker image so that the runner can access it in subsequent steps - Set the image name in docker-compose.yml to the same name as the bake-action ci.yml : # This is a basic workflow to help you get started with Actions name: CI # Controls when the workflow will run on: # Triggers the workflow on push or pull request events but only for the \"main\" branch push: branches: [ \"master\" ] pull_request: branches: [ \"master\" ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch: env: IMAGE_NAME: rdmolony/web PUSH_TAG: ${{ github.sha }} # A workflow run is made up of one or more jobs that can run sequentially or in parallel jobs: # This workflow contains a single job called \"build\" build: # The type of runner that the job will run on runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v3 # bake-action does not yet load .env - name: Export .env to GITHUB_ENV run: cat .env >> $GITHUB_ENV - name: Set up Docker Buildx uses: docker/setup-buildx-action@v2 - name: Build uses: docker/bake-action@v2.0.0 with: push: false load: true set: | web.cache-from=type=gha web.cache-to=type=gha web.tags=${{ env.IMAGE_NAME }} - name: Test run: docker compose run web docker-compose.yml : version: \"3.7\" services: web: build: context: . image: rdmolony/web command: echo \"Test run successful!\" Note : .env must be loaded into GITHUB_ENV to use the variables across steps Environment variables - GitHub Docs This action caches between workflow runs! This means that I won't have to rebuild if the image hasn't changed on the first pull request or push. I couldn't figure this out easily using satackey/action-docker-layer-caching though I imagine it's possible provided the right hash key is used. See Cache via bake action reuse by rdmolony \u00b7 Pull Request #6 \u00b7 rdmolony/cache-docker-compose-images (github.com)","title":"docker/bake-action"},{"location":"til/cache-docker-compose-images-via-github-actions/#github-actions","text":"","title":"github-actions"},{"location":"til/cache-docker-compose-images-via-github-actions/#docker","text":"","title":"docker"},{"location":"til/cache-docker-compose-images-via-github-actions/#docker-compose","text":"","title":"docker-compose"},{"location":"til/cache-package-manager-installs-with-buildkit/","text":"Cache package manager installs with buildkit \u00b6 Downloading project dependencies for every Docker rebuild is slow. buildkit enables caching downloads locally between builds thus skipping this download. Lifesaver. apt : # g++, unixodbc-dev for compiling pyodbc # python3-dev, default-libmysqlclient-dev, build-essential for compiling MySQL RUN --mount=type=cache,target=/var/cache/apt \\ --mount=type=cache,target=/var/lib/apt \\ apt-get update && apt-get install -y \\ g++ \\ python3-dev \\ build-essential poetry : COPY pyproject.toml poetry.lock /app/ COPY wheels/ /app/wheels/ RUN --mount=type=cache,target=/root/.cache \\ poetry install --no-dev --no-interaction See moby/buildkit docker \u00b6","title":"Cache package manager installs with buildkit"},{"location":"til/cache-package-manager-installs-with-buildkit/#cache-package-manager-installs-with-buildkit","text":"Downloading project dependencies for every Docker rebuild is slow. buildkit enables caching downloads locally between builds thus skipping this download. Lifesaver. apt : # g++, unixodbc-dev for compiling pyodbc # python3-dev, default-libmysqlclient-dev, build-essential for compiling MySQL RUN --mount=type=cache,target=/var/cache/apt \\ --mount=type=cache,target=/var/lib/apt \\ apt-get update && apt-get install -y \\ g++ \\ python3-dev \\ build-essential poetry : COPY pyproject.toml poetry.lock /app/ COPY wheels/ /app/wheels/ RUN --mount=type=cache,target=/root/.cache \\ poetry install --no-dev --no-interaction See moby/buildkit","title":"Cache package manager installs with buildkit"},{"location":"til/cache-package-manager-installs-with-buildkit/#docker","text":"","title":"docker"},{"location":"til/cache-wget-installs-with-buildkit/","text":"How to cache wget installs with buildkit \u00b6 I want to use Buildkit cache to save files downloaded via wget . I can skip a downloading a file using -nc / --no-clobber to skip overwriting a file that already exists but I cannot use this alongside -O / --output-document if I want to specify the filename. I also cannot use the more intelligent -N / --timestamping which also checks if the local version is newer than the remote version for the same reason. I can, however, manually check a file exists with test ... test -f /root/.wheels/basemap-1.2.2rel.tar.gz || \\ wget -O /root/.wheels/basemap-1.2.2rel.tar.gz https://github.com/matplotlib/basemap/archive/refs/tags/v1.2.2rel.tar.gz Now to use this alongside Buildkit cache. Mimicking package managers, I can save the file to /root/.wheels and copy it across ... RUN --mount=type=cache,target=/root/.wheels \\ mkdir /app/wheels && \\ test -f /root/.wheels/basemap-1.2.2rel.tar.gz || \\ wget -O /root/.wheels/basemap-1.2.2rel.tar.gz https://github.com/matplotlib/basemap/archive/refs/tags/v1.2.2rel.tar.gz && \\ cp /root/.wheels/basemap-1.2.2rel.tar.gz /app/wheels/basemap-1.2.2rel.tar.gz docker \u00b6","title":"How to cache wget installs with buildkit"},{"location":"til/cache-wget-installs-with-buildkit/#how-to-cache-wget-installs-with-buildkit","text":"I want to use Buildkit cache to save files downloaded via wget . I can skip a downloading a file using -nc / --no-clobber to skip overwriting a file that already exists but I cannot use this alongside -O / --output-document if I want to specify the filename. I also cannot use the more intelligent -N / --timestamping which also checks if the local version is newer than the remote version for the same reason. I can, however, manually check a file exists with test ... test -f /root/.wheels/basemap-1.2.2rel.tar.gz || \\ wget -O /root/.wheels/basemap-1.2.2rel.tar.gz https://github.com/matplotlib/basemap/archive/refs/tags/v1.2.2rel.tar.gz Now to use this alongside Buildkit cache. Mimicking package managers, I can save the file to /root/.wheels and copy it across ... RUN --mount=type=cache,target=/root/.wheels \\ mkdir /app/wheels && \\ test -f /root/.wheels/basemap-1.2.2rel.tar.gz || \\ wget -O /root/.wheels/basemap-1.2.2rel.tar.gz https://github.com/matplotlib/basemap/archive/refs/tags/v1.2.2rel.tar.gz && \\ cp /root/.wheels/basemap-1.2.2rel.tar.gz /app/wheels/basemap-1.2.2rel.tar.gz","title":"How to cache wget installs with buildkit"},{"location":"til/cache-wget-installs-with-buildkit/#docker","text":"","title":"docker"},{"location":"til/cascade-on-delete/","text":"Automatically delete child tables entries when the parent table entry is deleted. If have tables building and rooms linked by a building_no foreign key then all rooms in building X are deleted from rooms when this building is deleted! https://www.mysqltutorial.org/mysql-on-delete-cascade/ sql \u00b6 til \u00b6","title":"Cascade on delete"},{"location":"til/cascade-on-delete/#sql","text":"","title":"sql"},{"location":"til/cascade-on-delete/#til","text":"","title":"til"},{"location":"til/checkout-submodules-in-github-actions/","text":"I wanted to store til markdown files in Obsidian markdown file in a dedicated repository & link to an mkdocs static site via git submodule . It turns out that actions/checkout@v2 does not automatically fetch submodules! I had to ask it to do so via ... ... - uses: actions/checkout@v2 with: submodules: true ... mkdocs \u00b6 github-actions \u00b6","title":"Checkout submodules in github actions"},{"location":"til/checkout-submodules-in-github-actions/#mkdocs","text":"","title":"mkdocs"},{"location":"til/checkout-submodules-in-github-actions/#github-actions","text":"","title":"github-actions"},{"location":"til/commit-per-file-via-sh/","text":"I have multiple new files in til that I have not yet committed to source control. I'd like to commit each individually with a generic name Add <FILENAME> I'd also like each update too look like Update <FILEPATH> I can get a list of all files via ... git status --short # ?? <FILENAME> ... and select only untracked files via --untracked or via grep ... git status --short | grep \"??\" # <FILENAME> ... and remove the string ?? via sed ... git status --short | grep \"??\" | sed 's/?? //\" # <FILENAME> ... and loop through and commit via xargs ... # Commit all untracked files as 'Add <FILENAME>' git status --short --untracked-files \\ | grep \"??\" \\ | sed \"s/?? //\" \\ | xargs -I file sh -c 'git add file && git commit -m \"Add file\"' ... where -I means replace-str i.e. replace occurrences of replace-str in the initial-arguments with names read from standard input. I can modify this slightly to update or delete each file via ... # Commit all modified files as 'Modify <FILENAME>' git status --short --untracked-files=no \\ | grep \"M \" \\ | sed \"s/M //\" \\ | xargs -I file sh -c 'git add file && git commit -m \"Modify file\"' shell \u00b6 git \u00b6","title":"Commit per file via sh"},{"location":"til/commit-per-file-via-sh/#shell","text":"","title":"shell"},{"location":"til/commit-per-file-via-sh/#git","text":"","title":"git"},{"location":"til/compare-files-for-equality-in-python/","text":"I tried ... import filecmp left = \"<FILEPATH>\" right = \"<FILEPATH\" assert filecmp.cmp(left, right) ... but this proved to be flakey across test runs... Does cmp also compare file metadata? I tried ... left = \"<FILEPATH>\" right = \"<FILEPATH\" with open(left) as l, open(right) as r assert left.read() == right.read() ... but this takes an age to run & doesn't scale as it loads each file into memory! Finally, this Stackoverflow functional-style approach seemed to work fine ... def assert_files_are_equivalent(left: os.PathLike, right: os.PathLike): # https://stackoverflow.com/questions/254350/in-python-is-there-a-concise-way-of-comparing-whether-the-contents-of-two-text with open(left, \"rb\") as l, open(right, \"rb\") as r: assert ( os.fstat(l.fileno()).st_size == os.fstat(r.fileno()).st_size, \"Files are different sizes!\" ) # set up one 4k-reader for each file l_reader= functools.partial(l.read, 4096) r_reader= functools.partial(r.read, 4096) # pair each 4k-chunk from the two readers while they do not return '' (EOF) cmp_pairs= zip(iter(l_reader, b''), iter(r_reader, b'')) # return True for all pairs that are not equal # voil\u00e0; any() stops at first True value assert ( not any(itertools.starmap(operator.ne, cmp_pairs)), \"File content differs!\" ) In Python, is there a concise way of comparing whether the contents of two text files are the same? - Stack Overflow","title":"Compare files for equality in python"},{"location":"til/connect-to-mssql-db-via-sqlalchemy/","text":"Connect to mssql db via sqlalchemy \u00b6 I want to upload csv data to a Microsoft SQL Server ( mssql ) database. I can use pandas to read & wrangle the csv data and sqlalchemy and pyodbc to upload it. To authenticate my flow I can pass in my credentials using a config dict loaded from a .env file (or prefect secrets) ... import sqlalchemy as sa connection_string = ( \"mssql+pyodbc://\" f\"{config['MSSQL_USER']}:{config['MSSQL_PASSWORD']}\" f\"@{config['MSSQL_HOST']}:{config['MSSQL_PORT']}\" f\"/{db_name}\" f\"?driver={config['MSSQL_DRIVER']}\" ) engine = sa.create_engine(connection_string) table_name = \"my_table\" df.to_sql(name=table_name, con=engine, if_exists=\"append\") ... where MSSQL_USER has the necessary permissions to create a new table if necessary. I can set these permissions using SQL Server Management Studio (SSMS) by: Adding a new user to Security > Logins with: User Mapping - add db_datareader , db_datawriter , db_ddladmin Securables - add server SERVER_NAME Vefifying user permissions on database via Databases > DATABASE_NAME > Properties > Permissions > Effective sql \u00b6","title":"Connect to mssql db via sqlalchemy"},{"location":"til/connect-to-mssql-db-via-sqlalchemy/#connect-to-mssql-db-via-sqlalchemy","text":"I want to upload csv data to a Microsoft SQL Server ( mssql ) database. I can use pandas to read & wrangle the csv data and sqlalchemy and pyodbc to upload it. To authenticate my flow I can pass in my credentials using a config dict loaded from a .env file (or prefect secrets) ... import sqlalchemy as sa connection_string = ( \"mssql+pyodbc://\" f\"{config['MSSQL_USER']}:{config['MSSQL_PASSWORD']}\" f\"@{config['MSSQL_HOST']}:{config['MSSQL_PORT']}\" f\"/{db_name}\" f\"?driver={config['MSSQL_DRIVER']}\" ) engine = sa.create_engine(connection_string) table_name = \"my_table\" df.to_sql(name=table_name, con=engine, if_exists=\"append\") ... where MSSQL_USER has the necessary permissions to create a new table if necessary. I can set these permissions using SQL Server Management Studio (SSMS) by: Adding a new user to Security > Logins with: User Mapping - add db_datareader , db_datawriter , db_ddladmin Securables - add server SERVER_NAME Vefifying user permissions on database via Databases > DATABASE_NAME > Properties > Permissions > Effective","title":"Connect to mssql db via sqlalchemy"},{"location":"til/connect-to-mssql-db-via-sqlalchemy/#sql","text":"","title":"sql"},{"location":"til/create-a-test-mysql-database-via-pytest-django/","text":"Create a test mysql database via pytest-django \u00b6 I want to use pytest-django to spin up a test MySQL database for every test run. By default pytest-django creates a test database called test_NAME , and the default USER does not have the necessary PRIVILEGES to create it. Yurii Halapup from this Stackoverflow thread recommends explictely naming the test database in settings.py ... DATABASES = { 'default': { ... 'TEST': { 'NAME': 'test_finance', }, } } ... starting the MySQL shell ... mysql -u root -p ... and running ... GRANT ALL PRIVILEGES ON test_NAME.* TO 'USER'@'HOST'; The test database creation now works as expected django \u00b6","title":"Create a test mysql database via pytest-django"},{"location":"til/create-a-test-mysql-database-via-pytest-django/#create-a-test-mysql-database-via-pytest-django","text":"I want to use pytest-django to spin up a test MySQL database for every test run. By default pytest-django creates a test database called test_NAME , and the default USER does not have the necessary PRIVILEGES to create it. Yurii Halapup from this Stackoverflow thread recommends explictely naming the test database in settings.py ... DATABASES = { 'default': { ... 'TEST': { 'NAME': 'test_finance', }, } } ... starting the MySQL shell ... mysql -u root -p ... and running ... GRANT ALL PRIVILEGES ON test_NAME.* TO 'USER'@'HOST'; The test database creation now works as expected","title":"Create a test mysql database via pytest-django"},{"location":"til/create-a-test-mysql-database-via-pytest-django/#django","text":"","title":"django"},{"location":"til/deploy-docker-compose-images-via-github-actions/","text":"I setup an action to test pull requests & pushes to the main branch ( cache-docker-compose-images-via-github-actions ). This action saves all system dependencies to an image & volume mounts the source code to it. This means that caching across workflow runs is simple and fast. It takes roughly 30 seconds instead to setup the environment instead of ~5 minutes. I then wanted a separate action to deploy updated source code (i.e. pushes to main ) to Docker Hub where it can be accessed by Azure Web App to rebuild our production web app. Amazingly bake-action is able to use the cached image layers from the tests.yml action to build the image! The only difference between this action and tests.yml is that we use poetry install --no-dev for production, and we copy the source code across into the image. The only blocker was adapting bake-action to save images at their git commit SHA (a unique identifier). bake-action set doesn't work with environment variables if the environment variables themselves consists of environment variables - env: IMAGE_NAME: rdmolony/stationmanager-dev:${{ github.sha }} jobs: build: runs-on: ubuntu-latest steps: ... - name: Build uses: docker/bake-action@v2.0.0 with: push: false load: true set: | web.cache-from=type=gha web.cache-to=type=gha web.tags=${{ env.IMAGE_NAME }} I can instead define an image name using the git commit SHA in compose - - name: Build uses: docker/bake-action@v2.0.0 env: TARGET: production IMAGE_NAME: rdmolony/stationmanager:${{ github.sha }} with: push: true set: | web.cache-from=type=gha services: web: environment: - TARGET - IMAGE_NAME build: context: . target: $TARGET image: $IMAGE_NAME github-actions \u00b6 docker \u00b6 docker-compose \u00b6","title":"Deploy docker compose images via github actions"},{"location":"til/deploy-docker-compose-images-via-github-actions/#github-actions","text":"","title":"github-actions"},{"location":"til/deploy-docker-compose-images-via-github-actions/#docker","text":"","title":"docker"},{"location":"til/deploy-docker-compose-images-via-github-actions/#docker-compose","text":"","title":"docker-compose"},{"location":"til/estimate-wind-speed-forecast-uncertainty/","text":"Estimate wind speed forecast uncertainty \u00b6 Measurement accuracy \u00b6 On-site wind monitoring \u00b6 Type Description Causes Estimate? Reduce? Instrument accuracy - Calibration and mounting arrangement of the instruments ~2-3% ~0.2-0.3% by calibration Measurement interference - Anemometer mounting, type of tower, echoes from nearby objects - - Measurement inconsistency - Icing or equipment malfunction - - Second-order effects - Over-speeding, degradation or air density variations - - Long-term wind resource extrapolation \u00b6 Also known as long-term measurement higher wind regime Type Description Causes Estimate? Reduce? On-site data synthesis - Strength of the correlations between mast locations, amount of data synthesised - - Consistency of reference data - Level, nature & metadata of regional validation available - - Correlation to reference station - - - - Representativeness of period of data - How well the record period represents the long-term wind conditions inter-annual variability / SQRT(years of measurement data) - Historical wind frequency distribution - Uncertainty of the wind speed distribution measured over the period of data collected 2% / SQRT(years of measurement data) - Other \u00b6 Type Description Causes Estimate? Reduce? Vertical wind resource extrapolation Hub height shear values estimated up from mast values Representativeness of masts and heights, consistency of shear between towers, atmospheric stability and measurement configurations - - Input data accuracy - Validity of the topographic map, land cover map, position and height of obstacles to the flow as windbreaks for both the historical and future period of operation - - Spatial wind resource extrapolation Turbine values estimated across from mast values Representativeness of measurement locations vs turbine locations, complexity of the wind flow at the site (variations in ground cover) model vs measured wind speeds - Loss factor uncertainty \u00b6 Also known as energy production analysis Type Description Causes Estimate? Reduce? Wakes - Representativeness of model normal distribution with a standard deviation of ~25-35% of the wake effect - Availability Availability of the wind turbine, substation & electrical grid \"\" weibull distribution with a standard deviation of 3% - Electrical - \"\" normal distribution with a standard deviation of 10% of the loss - Turbine performance - \"\" normal distribution with a standard deviation of 2-3% of the loss - Environmental - \"\" normal distribution with a standard deviation of 10% of the loss - Curtailment - \"\" normal distribution with a standard deviation of 10% of the loss - Inter-annual variability \u00b6 Type Description Causes Estimate? Reduce? Future wind frequency distribution Year-to-year variability of wind speed distribution and air density - ~2% - Inter-annual variability Year-to-year variation of average wind speed - inter-annual variation / SQRT(10), ~6% Take a longer time-period wind-energy \u00b6","title":"Estimate wind speed forecast uncertainty"},{"location":"til/estimate-wind-speed-forecast-uncertainty/#estimate-wind-speed-forecast-uncertainty","text":"","title":"Estimate wind speed forecast uncertainty"},{"location":"til/estimate-wind-speed-forecast-uncertainty/#measurement-accuracy","text":"","title":"Measurement accuracy"},{"location":"til/estimate-wind-speed-forecast-uncertainty/#on-site-wind-monitoring","text":"Type Description Causes Estimate? Reduce? Instrument accuracy - Calibration and mounting arrangement of the instruments ~2-3% ~0.2-0.3% by calibration Measurement interference - Anemometer mounting, type of tower, echoes from nearby objects - - Measurement inconsistency - Icing or equipment malfunction - - Second-order effects - Over-speeding, degradation or air density variations - -","title":"On-site wind monitoring"},{"location":"til/estimate-wind-speed-forecast-uncertainty/#long-term-wind-resource-extrapolation","text":"Also known as long-term measurement higher wind regime Type Description Causes Estimate? Reduce? On-site data synthesis - Strength of the correlations between mast locations, amount of data synthesised - - Consistency of reference data - Level, nature & metadata of regional validation available - - Correlation to reference station - - - - Representativeness of period of data - How well the record period represents the long-term wind conditions inter-annual variability / SQRT(years of measurement data) - Historical wind frequency distribution - Uncertainty of the wind speed distribution measured over the period of data collected 2% / SQRT(years of measurement data) -","title":"Long-term wind resource extrapolation"},{"location":"til/estimate-wind-speed-forecast-uncertainty/#other","text":"Type Description Causes Estimate? Reduce? Vertical wind resource extrapolation Hub height shear values estimated up from mast values Representativeness of masts and heights, consistency of shear between towers, atmospheric stability and measurement configurations - - Input data accuracy - Validity of the topographic map, land cover map, position and height of obstacles to the flow as windbreaks for both the historical and future period of operation - - Spatial wind resource extrapolation Turbine values estimated across from mast values Representativeness of measurement locations vs turbine locations, complexity of the wind flow at the site (variations in ground cover) model vs measured wind speeds -","title":"Other"},{"location":"til/estimate-wind-speed-forecast-uncertainty/#loss-factor-uncertainty","text":"Also known as energy production analysis Type Description Causes Estimate? Reduce? Wakes - Representativeness of model normal distribution with a standard deviation of ~25-35% of the wake effect - Availability Availability of the wind turbine, substation & electrical grid \"\" weibull distribution with a standard deviation of 3% - Electrical - \"\" normal distribution with a standard deviation of 10% of the loss - Turbine performance - \"\" normal distribution with a standard deviation of 2-3% of the loss - Environmental - \"\" normal distribution with a standard deviation of 10% of the loss - Curtailment - \"\" normal distribution with a standard deviation of 10% of the loss -","title":"Loss factor uncertainty"},{"location":"til/estimate-wind-speed-forecast-uncertainty/#inter-annual-variability","text":"Type Description Causes Estimate? Reduce? Future wind frequency distribution Year-to-year variability of wind speed distribution and air density - ~2% - Inter-annual variability Year-to-year variation of average wind speed - inter-annual variation / SQRT(10), ~6% Take a longer time-period","title":"Inter-annual variability"},{"location":"til/estimate-wind-speed-forecast-uncertainty/#wind-energy","text":"","title":"wind-energy"},{"location":"til/filter-django-models-via-null-fields/","text":"I can find all objects where a specific field is not null or empty via <object>.objects.filter(corewinddata__isnull=False) python \u00b6 django \u00b6","title":"Filter django models via null fields"},{"location":"til/filter-django-models-via-null-fields/#python","text":"","title":"python"},{"location":"til/filter-django-models-via-null-fields/#django","text":"","title":"django"},{"location":"til/finding-the-first-true-value-in-a-list-via-python/","text":"I want to check a directory for a potential list of subfolders & if it exists select it. I can check if it exists like ... subfolders = [ \"Generic_calibration\", \"Generic Calibration\", \"Table10m\", \"TenMin\", ] subfolder_exists = any(filesystem.exists(sfp) for sfp in subfolders) ... and get the first subfolder that exists via ... if subfolder_exists: subfolder = next(sfp for sfp in subfolder_paths if filesystem.exists(sfp)) else: subfolder = station_dir I could do this more efficiently via ... subfolder = next( (sfp for sfp in subfolder_paths if filesystem.exists(sfp)). station_dir ) ... where station_dir is the default value if the generator is exausted, but it doesn't read as nicely! python how can return first value = true in a list? - Stack Overflow python \u00b6 functional-programming \u00b6","title":"Finding the first true value in a list via python"},{"location":"til/finding-the-first-true-value-in-a-list-via-python/#python","text":"","title":"python"},{"location":"til/finding-the-first-true-value-in-a-list-via-python/#functional-programming","text":"","title":"functional-programming"},{"location":"til/flag-wind-turbine-timestamps-in-fault-time-intervals/","text":"Flag wind turbine timestamps in fault time intervals \u00b6 I have turbine timeseries readings & turbine fault flags ... time ... 2017-12-31 17:00:00 ... 2017-12-31 17:10:00 ... 2017-12-31 17:20:00 ... 2017-12-31 17:30:00 ... ... fault_start_time fault_end_time ... ... 2017-12-31 17:02:00 2017-12-31 17:12:00 ... ... and I want to know if there has been any wind turbine faults during any 10 minute time interval time ... turbine_fault_occurs 2017-12-31 17:00:00 ... False 2017-12-31 17:10:00 ... True 2017-12-31 17:20:00 ... True 2017-12-31 17:30:00 ... False How do I adapt my fault time interval into the same 10 minute interval as my timeseries data? Lets first code these expectations as a unit test on which we can test our implementation ... import pandas as pd from pandas.testing import assert_frame_equal def test_flag_turbine_fault_occurences() -> None: timeseries = pd.DataFrame( { \"time\": [ \"2017-12-31 17:00:00\", \"2017-12-31 17:10:00\", \"2017-12-31 17:20:00\", \"2017-12-31 17:30:00\", ], }, dtype=\"datetime64[s]\", ) faults = pd.DataFrame( { \"fault_start_time\": [\"2017-12-31 17:02:00\"], \"fault_end_time\": [\"2017-12-31 17:12:00\"], }, dtype=\"datetime64[s]\", ) expected_output = pd.DataFrame( { \"time\": pd.Series( [ \"2017-12-31 17:00:00\", \"2017-12-31 17:10:00\", \"2017-12-31 17:20:00\", \"2017-12-31 17:30:00\", ], dtype=\"datetime64[s]\", ), \"turbine_fault_occurs\": [False, True, True, False], }, ) output = flag_turbine_fault_occurences( timeseries, faults, column_name_map={ \"time\": \"time\", \"fault_start_time\": \"fault_start_time\", \"fault_end_time\": \"fault_end_time\", }, ) assert_frame_equal(output, expected_output) Now for implementation ... We need to round each timestamp to the nearest 10 minute interval so fault_start_time 2017-12-31 17:02:00 becomes 2017-12-31 17:10:00 and fault_end_time 2017-12-31 17:12:00 becomes 2017-12-31 17:20:00 to capture the time interval in which either fault occurs. import pandas as pd from pandas.testing import assert_series_equal def test_ceil_to_nearest_10Min_interval() -> None: s = pd.Series([\"2017-12-31 17:02:00\", \"2017-12-31 17:58:00\"], dtype=\"datetime64[s]\") expected_output = pd.Series( [\"2017-12-31 17:10:00\", \"2017-12-31 18:00:00\"], dtype=\"datetime64[s]\" ) output = ceil_to_nearest_10Min_interval(s) assert_series_equal(output, expected_output) def ceil_to_nearest_10Min_interval(s: pd.Series) -> pd.Series: return s + pd.to_timedelta(-s.dt.minute % 10, unit='Min') Now we can call this ceil function to band the timestamps and link each start and end fault time with its corresponding time interval with a simple LEFT merge, which ensures we don't lose any time intervals for which no fault occurs. Lastly, we can check whether or not a fault begins or ends within each time interval and flag this with True or False ```python from typing import Dict import pandas as pd def flag_turbine_fault_occurences( timeseries: pd.DataFrame, faults: pd.DataFrame, column_name_map: Dict[str, str] ) -> pd.DataFrame: faults = faults.copy() c_time = column_name_map[\"time\"] c_fault_start_time = column_name_map[\"fault_start_time\"] c_fault_end_time = column_name_map[\"fault_end_time\"] fault_start_time_interval = ceil_to_nearest_10Min_interval( faults[c_fault_start_time] ) fault_end_time_interval = ceil_to_nearest_10Min_interval( faults[c_fault_end_time] ) timeseries_with_faults = ( timeseries.merge( fault_start_time_interval, left_on=c_time, right_on=c_fault_start_time, how=\"left\" ) .merge( fault_end_time_interval, left_on=c_time, right_on=c_fault_end_time, how=\"left\" ) ) timeseries_with_faults[\"turbine_fault_occurs\"] = ( (timeseries_with_faults[c_fault_start_time].notnull()) | (timeseries_with_faults[c_fault_end_time].notnull()) ) return timeseries_with_faults.drop(columns=[c_fault_start_time, c_fault_end_time]) ``` Inspired by how-to-join-two-dataframes-for-which-column-values-are-within-a-certain-range & merge-pandas-dataframes-where-one-value-is-between-two-others pandas \u00b6","title":"Flag wind turbine timestamps in fault time intervals"},{"location":"til/flag-wind-turbine-timestamps-in-fault-time-intervals/#flag-wind-turbine-timestamps-in-fault-time-intervals","text":"I have turbine timeseries readings & turbine fault flags ... time ... 2017-12-31 17:00:00 ... 2017-12-31 17:10:00 ... 2017-12-31 17:20:00 ... 2017-12-31 17:30:00 ... ... fault_start_time fault_end_time ... ... 2017-12-31 17:02:00 2017-12-31 17:12:00 ... ... and I want to know if there has been any wind turbine faults during any 10 minute time interval time ... turbine_fault_occurs 2017-12-31 17:00:00 ... False 2017-12-31 17:10:00 ... True 2017-12-31 17:20:00 ... True 2017-12-31 17:30:00 ... False How do I adapt my fault time interval into the same 10 minute interval as my timeseries data? Lets first code these expectations as a unit test on which we can test our implementation ... import pandas as pd from pandas.testing import assert_frame_equal def test_flag_turbine_fault_occurences() -> None: timeseries = pd.DataFrame( { \"time\": [ \"2017-12-31 17:00:00\", \"2017-12-31 17:10:00\", \"2017-12-31 17:20:00\", \"2017-12-31 17:30:00\", ], }, dtype=\"datetime64[s]\", ) faults = pd.DataFrame( { \"fault_start_time\": [\"2017-12-31 17:02:00\"], \"fault_end_time\": [\"2017-12-31 17:12:00\"], }, dtype=\"datetime64[s]\", ) expected_output = pd.DataFrame( { \"time\": pd.Series( [ \"2017-12-31 17:00:00\", \"2017-12-31 17:10:00\", \"2017-12-31 17:20:00\", \"2017-12-31 17:30:00\", ], dtype=\"datetime64[s]\", ), \"turbine_fault_occurs\": [False, True, True, False], }, ) output = flag_turbine_fault_occurences( timeseries, faults, column_name_map={ \"time\": \"time\", \"fault_start_time\": \"fault_start_time\", \"fault_end_time\": \"fault_end_time\", }, ) assert_frame_equal(output, expected_output) Now for implementation ... We need to round each timestamp to the nearest 10 minute interval so fault_start_time 2017-12-31 17:02:00 becomes 2017-12-31 17:10:00 and fault_end_time 2017-12-31 17:12:00 becomes 2017-12-31 17:20:00 to capture the time interval in which either fault occurs. import pandas as pd from pandas.testing import assert_series_equal def test_ceil_to_nearest_10Min_interval() -> None: s = pd.Series([\"2017-12-31 17:02:00\", \"2017-12-31 17:58:00\"], dtype=\"datetime64[s]\") expected_output = pd.Series( [\"2017-12-31 17:10:00\", \"2017-12-31 18:00:00\"], dtype=\"datetime64[s]\" ) output = ceil_to_nearest_10Min_interval(s) assert_series_equal(output, expected_output) def ceil_to_nearest_10Min_interval(s: pd.Series) -> pd.Series: return s + pd.to_timedelta(-s.dt.minute % 10, unit='Min') Now we can call this ceil function to band the timestamps and link each start and end fault time with its corresponding time interval with a simple LEFT merge, which ensures we don't lose any time intervals for which no fault occurs. Lastly, we can check whether or not a fault begins or ends within each time interval and flag this with True or False ```python from typing import Dict import pandas as pd def flag_turbine_fault_occurences( timeseries: pd.DataFrame, faults: pd.DataFrame, column_name_map: Dict[str, str] ) -> pd.DataFrame: faults = faults.copy() c_time = column_name_map[\"time\"] c_fault_start_time = column_name_map[\"fault_start_time\"] c_fault_end_time = column_name_map[\"fault_end_time\"] fault_start_time_interval = ceil_to_nearest_10Min_interval( faults[c_fault_start_time] ) fault_end_time_interval = ceil_to_nearest_10Min_interval( faults[c_fault_end_time] ) timeseries_with_faults = ( timeseries.merge( fault_start_time_interval, left_on=c_time, right_on=c_fault_start_time, how=\"left\" ) .merge( fault_end_time_interval, left_on=c_time, right_on=c_fault_end_time, how=\"left\" ) ) timeseries_with_faults[\"turbine_fault_occurs\"] = ( (timeseries_with_faults[c_fault_start_time].notnull()) | (timeseries_with_faults[c_fault_end_time].notnull()) ) return timeseries_with_faults.drop(columns=[c_fault_start_time, c_fault_end_time]) ``` Inspired by how-to-join-two-dataframes-for-which-column-values-are-within-a-certain-range & merge-pandas-dataframes-where-one-value-is-between-two-others","title":"Flag wind turbine timestamps in fault time intervals"},{"location":"til/flag-wind-turbine-timestamps-in-fault-time-intervals/#pandas","text":"","title":"pandas"},{"location":"til/git-push-periodically-via-cron/","text":"I want to sync my Obsidian notes with GitHub I can create a simple shell script to cd & git push ... #!/bin/sh/ cd '/mnt/c/Users/Rowan.Molony/OneDrive - Mainstream Renewable Power/Documents/Obsidian Vault' echo $(pwd) git add . git commit -m \"$(date)\" git push ... and run via crontab by editing its entries with crontab -e and adding ... 00 09 * * 1-5 sh ~/Code/cronjobs/push_obsidian_notes_to_github.sh ... where ... * * * * * command to be executed - - - - - | | | | | | | | | ----- Day of week (0 - 7) (Sunday=0 or 7) | | | ------- Month (1 - 12) | | --------- Day of month (1 - 31) | ----------- Hour (0 - 23) ------------- Minute (0 - 59) How To Add Jobs To cron Under Linux or UNIX - nixCraft (cyberciti.biz) Crontab.guru - The cron schedule expression editor Crontab MAILTO Parameter to Send Notification (linuxhint.com) Crontab Explained in Linux [With Examples] (linuxhandbook.com)","title":"Git push periodically via cron"},{"location":"til/index-last-month-via-pandas/","text":"I have a timeseries ... \"TmStamp\",\"RecNum\",\"WS_80m_90deg_Avg\",... 2021-06-29 00:00:00.000,459742,3.245,... ... ... and I want to grab the last month via pandas . I can do this easily using pandas.DateOffset ! offset = myseries.index[-1] - pd.DateOffset(hours=5) myseries.loc[offset::] python - Previous month datetime pandas - Stack Overflow","title":"Index last month via pandas"},{"location":"til/install-packages-on-shell-startup/","text":"I wanted to adapt my zshrc so that oh-my-zsh , powerlevel10k , conda & nix install on startup I can check for directories via -d ... if ! [ -d \"$HOME/.oh-my-zsh\" ]; then git clone --depth=1 https://github.com/ohmyzsh/ohmyzsh \"$HOME/.oh-my-zsh\" fi I tried to check for commands via command -v MYCOMMAND ... if ! [ -x \"$(command -v nix)\" ]; then sh <(curl -L https://nixos.org/nix/install) --no-daemon fi ... however, this reinstalled the packages on every startup! Why? Not a big deal. I can just use the same directory check; .nix-profiles for nix and mambaforge for conda ! How can I check if a program exists from a Bash script? - Stack Overflow date/2022-07-11 \u00b6 til \u00b6 shell \u00b6","title":"Install packages on shell startup"},{"location":"til/install-packages-on-shell-startup/#date2022-07-11","text":"","title":"date/2022-07-11"},{"location":"til/install-packages-on-shell-startup/#til","text":"","title":"til"},{"location":"til/install-packages-on-shell-startup/#shell","text":"","title":"shell"},{"location":"til/install-python-packages-on-ubuntu-with-c-dependencies/","text":"Install python packages on ubuntu with c dependencies \u00b6 A C-compiler is sometimes required to pip install Python packages. I'm trying to pip install StationManager dependencies from a frozen requirements.txt from within a python:3.7-slim Docker container which doesn't come with the required compiler gcc . Frustratingly it's not as simple as adding the line apt-get install -y gcc to the Dockerfile to install this to the system before attempting this pip install . Some deeper digging reveals that pyodbc is where the issue lies ... ... Running setup.py install for pyodbc: finished with status 'error' ... gcc: fatal error: cannot execute \u2018cc1plus\u2019: execvp: No such file or directory compilation terminated. error: command 'gcc' failed with exit status 1 ... A bit of searching reveals that g++ not gcc is required ( #165 mkleehammer/pyodbc ) because the problematic file is a C++ file not a C file so gcc needs cc1plus ! Adding the suggested lines fixes the issue: FROM python:3.7-slim ENV PYTHONUNBUFFERED=1 WORKDIR /code RUN apt-get update && apt-get install -y \\ g++ \\ unixodbc-dev \\ git COPY requirements.txt /code/ RUN pip install -r requirements.txt COPY . /code/ docker \u00b6","title":"Install python packages on ubuntu with c dependencies"},{"location":"til/install-python-packages-on-ubuntu-with-c-dependencies/#install-python-packages-on-ubuntu-with-c-dependencies","text":"A C-compiler is sometimes required to pip install Python packages. I'm trying to pip install StationManager dependencies from a frozen requirements.txt from within a python:3.7-slim Docker container which doesn't come with the required compiler gcc . Frustratingly it's not as simple as adding the line apt-get install -y gcc to the Dockerfile to install this to the system before attempting this pip install . Some deeper digging reveals that pyodbc is where the issue lies ... ... Running setup.py install for pyodbc: finished with status 'error' ... gcc: fatal error: cannot execute \u2018cc1plus\u2019: execvp: No such file or directory compilation terminated. error: command 'gcc' failed with exit status 1 ... A bit of searching reveals that g++ not gcc is required ( #165 mkleehammer/pyodbc ) because the problematic file is a C++ file not a C file so gcc needs cc1plus ! Adding the suggested lines fixes the issue: FROM python:3.7-slim ENV PYTHONUNBUFFERED=1 WORKDIR /code RUN apt-get update && apt-get install -y \\ g++ \\ unixodbc-dev \\ git COPY requirements.txt /code/ RUN pip install -r requirements.txt COPY . /code/","title":"Install python packages on ubuntu with c dependencies"},{"location":"til/install-python-packages-on-ubuntu-with-c-dependencies/#docker","text":"","title":"docker"},{"location":"til/installing-powershell-modules-on-first-launch/","text":"Installing powershell modules on first launch \u00b6 I wanted to setup my $PROFILE so that any external modules ( oh-my-posh , posh-git and powershell-git-aliases ) install on first launch. I added a simple if statement to the top of my profile ( here ) if (-Not (Get-Module -Name oh-my-posh)) { Install-Module oh-my-posh -Scope CurrentUser } if (-Not (Get-Module -Name git-aliases)) { Install-Module git-aliases -Scope CurrentUser } This creates two problems: If one of the modules is not at the latest version, powershell tries to install it alongside the existing version in a new directory alongside the old version so I then have oh-my-posh > 5.9.0 > 5.12.1 For whatever reason it's a really really slow operation to run on every startup: 16888ms vs 1892ms For now I'm keeping all external modules as a comment at the bottom of my profile :exclamation: See profile here powershell \u00b6","title":"Installing powershell modules on first launch"},{"location":"til/installing-powershell-modules-on-first-launch/#installing-powershell-modules-on-first-launch","text":"I wanted to setup my $PROFILE so that any external modules ( oh-my-posh , posh-git and powershell-git-aliases ) install on first launch. I added a simple if statement to the top of my profile ( here ) if (-Not (Get-Module -Name oh-my-posh)) { Install-Module oh-my-posh -Scope CurrentUser } if (-Not (Get-Module -Name git-aliases)) { Install-Module git-aliases -Scope CurrentUser } This creates two problems: If one of the modules is not at the latest version, powershell tries to install it alongside the existing version in a new directory alongside the old version so I then have oh-my-posh > 5.9.0 > 5.12.1 For whatever reason it's a really really slow operation to run on every startup: 16888ms vs 1892ms For now I'm keeping all external modules as a comment at the bottom of my profile :exclamation: See profile here","title":"Installing powershell modules on first launch"},{"location":"til/installing-powershell-modules-on-first-launch/#powershell","text":"","title":"powershell"},{"location":"til/login-to-a-django-app-via-selenium-and-pytest-django/","text":"Login to a django app via selenium and pytest django \u00b6 I wanted to run end to end browser tests using Selenium and pytest-django . I struggled to adapt marcgibbons/django-selenium-docker from unittest to pytest . I couldn't authenticate the automatically authenticated user or admin_user generated by a pytest-django pytest fixture. I was able to verify that pytest-django does in fact create an authenticated user by dropping a breakpoint into my functional test ... from django.contrib.auth import authenticate from django.contrib.auth.models import User @pytest.mark.django_db(transaction=True) def test_manual_admin_user_login( live_server_url: str, browser: webdriver.Remote, admin_user: User ) -> None: breakpoint() >> User.objects.all() >> authenticate(user=\"admin\", password=\"password\") ... so the issue wasn't pytest-django user creation. I tried to manually login to the website as @marcgibbons does ... from urllib.parse import urlparse from django.conf import Settings from django.contrib.auth.models import User import pytest from pytest_django.live_server_helper import LiveServer from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities @pytest.fixture(autouse=True) def override_allowed_hosts(settings: Settings) -> None: settings.ALLOWED_HOSTS = [\"*\"] # Disable ALLOW_HOSTS @pytest.fixture def live_server_url() -> str: # Set host to externally accessible web server address return str(LiveServer(addr=\"django\")) @pytest.fixture def browser() -> webdriver.Remote: browser_ = webdriver.Remote( command_executor='http://selenium:4444/wd/hub', desired_capabilities=DesiredCapabilities.CHROME, ) yield browser_ browser_.quit() @pytest.mark.django_db def test_manual_admin_user_login( live_server_url: str, browser: webdriver.Remote, admin_user: User ) -> None: \"\"\" As a superuser with valid credentials, I should gain access to the Django admin. \"\"\" browser.get(live_server_url) username_input = browser.find_element_by_name('username') username_input.send_keys('admin') password_input = browser.find_element_by_name('password') password_input.send_keys('password') browser.find_element_by_xpath('//input[@value=\"Log in\"]').click() path = urlparse(browser.current_url).path assert path == '/' body_text = browser.find_element_by_tag_name('body').text assert 'WELCOME, ADMIN.' in body_text ... this didn't work either. I tried overriding my browser's cookies with the admin_user fixture credentials ... Adapted from @aljosa implementation in an issue on the pytest-django GitHub from urllib.parse import urlparse from django.conf import settings from django.conf import Settings from django.test.client import Client from django.contrib.auth.models import User import pytest from pytest_django.live_server_helper import LiveServer from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities @pytest.fixture def authenticated_browser( admin_client: Client, browser: webdriver.Remote, live_server_url: str ) -> webdriver.Remote: browser.get(live_server_url) sessionid = admin_client.cookies[\"sessionid\"] cookie = { 'name': settings.SESSION_COOKIE_NAME, 'value': sessionid.value, 'path': '/' } browser.add_cookie(cookie) browser.refresh() return browser @pytest.mark.django_db def test_auto_admin_user_login( live_server_url: str, authenticated_browser: webdriver.Remote, admin_user: User ) -> None: browser = authenticated_browser browser.get(live_server_url) path = urlparse(browser.current_url).path assert path == '/' body_text = browser.find_element_by_tag_name('body').text assert 'WELCOME, ADMIN.' in body_text ... this failed too! It turns out that the user credentials must be stored in the django website database or else it won't be able to access them! This means that we need to allow pytest-django to migrate the credentials into the test database in order to login via selenium. I can just replace ... @pytest.mark.django_db ... with ... @pytest.mark.django_db(transaction=True) ... and I can login as expected docker \u00b6","title":"Login to a django app via selenium and pytest django"},{"location":"til/login-to-a-django-app-via-selenium-and-pytest-django/#login-to-a-django-app-via-selenium-and-pytest-django","text":"I wanted to run end to end browser tests using Selenium and pytest-django . I struggled to adapt marcgibbons/django-selenium-docker from unittest to pytest . I couldn't authenticate the automatically authenticated user or admin_user generated by a pytest-django pytest fixture. I was able to verify that pytest-django does in fact create an authenticated user by dropping a breakpoint into my functional test ... from django.contrib.auth import authenticate from django.contrib.auth.models import User @pytest.mark.django_db(transaction=True) def test_manual_admin_user_login( live_server_url: str, browser: webdriver.Remote, admin_user: User ) -> None: breakpoint() >> User.objects.all() >> authenticate(user=\"admin\", password=\"password\") ... so the issue wasn't pytest-django user creation. I tried to manually login to the website as @marcgibbons does ... from urllib.parse import urlparse from django.conf import Settings from django.contrib.auth.models import User import pytest from pytest_django.live_server_helper import LiveServer from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities @pytest.fixture(autouse=True) def override_allowed_hosts(settings: Settings) -> None: settings.ALLOWED_HOSTS = [\"*\"] # Disable ALLOW_HOSTS @pytest.fixture def live_server_url() -> str: # Set host to externally accessible web server address return str(LiveServer(addr=\"django\")) @pytest.fixture def browser() -> webdriver.Remote: browser_ = webdriver.Remote( command_executor='http://selenium:4444/wd/hub', desired_capabilities=DesiredCapabilities.CHROME, ) yield browser_ browser_.quit() @pytest.mark.django_db def test_manual_admin_user_login( live_server_url: str, browser: webdriver.Remote, admin_user: User ) -> None: \"\"\" As a superuser with valid credentials, I should gain access to the Django admin. \"\"\" browser.get(live_server_url) username_input = browser.find_element_by_name('username') username_input.send_keys('admin') password_input = browser.find_element_by_name('password') password_input.send_keys('password') browser.find_element_by_xpath('//input[@value=\"Log in\"]').click() path = urlparse(browser.current_url).path assert path == '/' body_text = browser.find_element_by_tag_name('body').text assert 'WELCOME, ADMIN.' in body_text ... this didn't work either. I tried overriding my browser's cookies with the admin_user fixture credentials ... Adapted from @aljosa implementation in an issue on the pytest-django GitHub from urllib.parse import urlparse from django.conf import settings from django.conf import Settings from django.test.client import Client from django.contrib.auth.models import User import pytest from pytest_django.live_server_helper import LiveServer from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities @pytest.fixture def authenticated_browser( admin_client: Client, browser: webdriver.Remote, live_server_url: str ) -> webdriver.Remote: browser.get(live_server_url) sessionid = admin_client.cookies[\"sessionid\"] cookie = { 'name': settings.SESSION_COOKIE_NAME, 'value': sessionid.value, 'path': '/' } browser.add_cookie(cookie) browser.refresh() return browser @pytest.mark.django_db def test_auto_admin_user_login( live_server_url: str, authenticated_browser: webdriver.Remote, admin_user: User ) -> None: browser = authenticated_browser browser.get(live_server_url) path = urlparse(browser.current_url).path assert path == '/' body_text = browser.find_element_by_tag_name('body').text assert 'WELCOME, ADMIN.' in body_text ... this failed too! It turns out that the user credentials must be stored in the django website database or else it won't be able to access them! This means that we need to allow pytest-django to migrate the credentials into the test database in order to login via selenium. I can just replace ... @pytest.mark.django_db ... with ... @pytest.mark.django_db(transaction=True) ... and I can login as expected","title":"Login to a django app via selenium and pytest django"},{"location":"til/login-to-a-django-app-via-selenium-and-pytest-django/#docker","text":"","title":"docker"},{"location":"til/map-django-models-one-to-one/","text":"I can create a new database table called CoreWindData (containing the timestamp of the most recently updated data) and map it to another table Station using a OneToOneField in django ... class CoreWindData(models.Model): station = models.OneToOneField(\"stationmanager.Station\", on_delete=CASCADE) ... I can then access CoreWindData objects from Station via <station>.corewinddata python \u00b6 django \u00b6","title":"Map django models one to one"},{"location":"til/map-django-models-one-to-one/#python","text":"","title":"python"},{"location":"til/map-django-models-one-to-one/#django","text":"","title":"django"},{"location":"til/mock-requesting-a-file-from-an-external-website-in-pytest/","text":"Mock requesting a file from an external website in pytest \u00b6 I'm developing a streamlit web application that requests a 1GB zip file from an external website, unzips it, cleans it, zips it and returns this new file to the user upon request. I want to mock out the call to this external website to encapsulate my functional & unit tests as it would be very inefficient to have to download this file on every test run. I first need to create a file BERPublicsearch.txt within a zip archive BERPublicsearch.zip ... @pytest.fixture def sample_bers(tmp_path: Path) -> BytesIO: bers = pd.read_csv(\"sample-BERPublicsearch.txt\", sep=\"\\t\") f = bers.to_csv(index=False, sep=\"\\t\") filepath = tmp_path / \"BERPublicsearch.zip\" with ZipFile(filepath, \"w\") as zf: zf.writestr(\"BERPublicsearch.txt\", f) return ZipFile(filepath).read(\"BERPublicsearch.txt\") ... where sample-BERPublicsearch.txt is a 100 row sample of the data set saved in source control and sample_bers returns a bytes representation of the zip file. I can now use the responses library to mock a request to the SEAI website and return this sample zip file ... from io import BytesIO import json from pathlib import Path from time import sleep from zipfile import ZipFile import pandas as pd import pytest import responses from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities with open(\"defaults.json\") as f: DEFAULTS = json.load(f) @responses.activate def test_user_can_download_default_bers( ..., sample_bers: BytesIO ) -> None: responses.add( responses.POST, DEFAULTS[\"download\"][\"url\"], body=sample_bers, content_type=\"application/x-zip-compressed\", headers={ \"content-disposition\": \"attachment; filename=BERPublicSearch.zip\" }, status=200, ) ... where all POST request arguments are saved in defaults.json . This could be improved by using pytest fixtures to encapsulate this logic in a function that could be called by any test to mock out this call to SEAI thus avoiding some duplication. pytest \u00b6","title":"Mock requesting a file from an external website in pytest"},{"location":"til/mock-requesting-a-file-from-an-external-website-in-pytest/#mock-requesting-a-file-from-an-external-website-in-pytest","text":"I'm developing a streamlit web application that requests a 1GB zip file from an external website, unzips it, cleans it, zips it and returns this new file to the user upon request. I want to mock out the call to this external website to encapsulate my functional & unit tests as it would be very inefficient to have to download this file on every test run. I first need to create a file BERPublicsearch.txt within a zip archive BERPublicsearch.zip ... @pytest.fixture def sample_bers(tmp_path: Path) -> BytesIO: bers = pd.read_csv(\"sample-BERPublicsearch.txt\", sep=\"\\t\") f = bers.to_csv(index=False, sep=\"\\t\") filepath = tmp_path / \"BERPublicsearch.zip\" with ZipFile(filepath, \"w\") as zf: zf.writestr(\"BERPublicsearch.txt\", f) return ZipFile(filepath).read(\"BERPublicsearch.txt\") ... where sample-BERPublicsearch.txt is a 100 row sample of the data set saved in source control and sample_bers returns a bytes representation of the zip file. I can now use the responses library to mock a request to the SEAI website and return this sample zip file ... from io import BytesIO import json from pathlib import Path from time import sleep from zipfile import ZipFile import pandas as pd import pytest import responses from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities with open(\"defaults.json\") as f: DEFAULTS = json.load(f) @responses.activate def test_user_can_download_default_bers( ..., sample_bers: BytesIO ) -> None: responses.add( responses.POST, DEFAULTS[\"download\"][\"url\"], body=sample_bers, content_type=\"application/x-zip-compressed\", headers={ \"content-disposition\": \"attachment; filename=BERPublicSearch.zip\" }, status=200, ) ... where all POST request arguments are saved in defaults.json . This could be improved by using pytest fixtures to encapsulate this logic in a function that could be called by any test to mock out this call to SEAI thus avoiding some duplication.","title":"Mock requesting a file from an external website in pytest"},{"location":"til/mock-requesting-a-file-from-an-external-website-in-pytest/#pytest","text":"","title":"pytest"},{"location":"til/monkeypatch-a-database-connection-in-pytest/","text":"How to monkeypatch a database connection \u00b6 I want to test that a method Datasource.fetch_dataframe raises a VPN connection error if the user is connected to StationManager but is not connected to a VPN. I'm not interested in connecting to a database, only in how this method reacts to a database connection error pyodbc.OperationalError . I tried using mock.MagicMock to replace pyodbc with a magic object that does nothing but return pyodbc.OperationalError when pyodbc.connect is called so that the database connection error is raised every time the test is run ... from unittest.mock import MagicMock import pyodbc from _pytest.monkeypatch import MonkeyPatch from stationmanager.models import Datasource def test_fetch_dataframe_raises_vpn_error(monkeypatch: MonkeyPatch) -> None: # ... other setup ... monkeypatch.setattr( pyodbc, \"connect\", MagicMock(return_value=pyodbc.OperationalError) ) datasource = Datasource() with pytest.raises(pyodbc.OperationalError): datasource.fetch_dataframe(raw_db_table=True) ... however this merely returns the pyodbc.OperationalError and does not raise an error. It must instead be raised explicitely ... def test_fetch_dataframe_raises_vpn_error(monkeypatch: MonkeyPatch) -> None: def _mock_pyodbc_connect(*args, **kwargs) -> None: raise pyodbc.OperationalError # ... other setup ... monkeypatch.setattr(pyodbc, \"connect\", _mock_pyodbc_connect) datasource = Datasource() with pytest.raises(pyodbc.OperationalError): datasource.fetch_dataframe(raw_db_table=True) pytest \u00b6","title":"How to monkeypatch a database connection"},{"location":"til/monkeypatch-a-database-connection-in-pytest/#how-to-monkeypatch-a-database-connection","text":"I want to test that a method Datasource.fetch_dataframe raises a VPN connection error if the user is connected to StationManager but is not connected to a VPN. I'm not interested in connecting to a database, only in how this method reacts to a database connection error pyodbc.OperationalError . I tried using mock.MagicMock to replace pyodbc with a magic object that does nothing but return pyodbc.OperationalError when pyodbc.connect is called so that the database connection error is raised every time the test is run ... from unittest.mock import MagicMock import pyodbc from _pytest.monkeypatch import MonkeyPatch from stationmanager.models import Datasource def test_fetch_dataframe_raises_vpn_error(monkeypatch: MonkeyPatch) -> None: # ... other setup ... monkeypatch.setattr( pyodbc, \"connect\", MagicMock(return_value=pyodbc.OperationalError) ) datasource = Datasource() with pytest.raises(pyodbc.OperationalError): datasource.fetch_dataframe(raw_db_table=True) ... however this merely returns the pyodbc.OperationalError and does not raise an error. It must instead be raised explicitely ... def test_fetch_dataframe_raises_vpn_error(monkeypatch: MonkeyPatch) -> None: def _mock_pyodbc_connect(*args, **kwargs) -> None: raise pyodbc.OperationalError # ... other setup ... monkeypatch.setattr(pyodbc, \"connect\", _mock_pyodbc_connect) datasource = Datasource() with pytest.raises(pyodbc.OperationalError): datasource.fetch_dataframe(raw_db_table=True)","title":"How to monkeypatch a database connection"},{"location":"til/monkeypatch-a-database-connection-in-pytest/#pytest","text":"","title":"pytest"},{"location":"til/monkeypatch-functions-with-mock/","text":"I can use Mock to record calls & return a value in pytest if I want to replace a function ... from unittest import Mock import fsspec def test_foo(monkeypatch): monkeypatch.setattr( \"stationmanager.tasks.fsspec.filesystem\", Mock(return_value=fsspec.filesystem(\"file\")) ) ... Specifically, I wanted to replace an smb based filesystem with a local filesystem for testing! python \u00b6 pytest \u00b6","title":"Monkeypatch functions with mock"},{"location":"til/monkeypatch-functions-with-mock/#python","text":"","title":"python"},{"location":"til/monkeypatch-functions-with-mock/#pytest","text":"","title":"pytest"},{"location":"til/override-django-huey-immediate-mode/","text":"How to run djhuey tasks in immediate mode \u00b6 I want to test some huey tasks. To do so I need to adapt them into immediate mode. This is complicated by the fact that my tasks are created by decorating functions with db_task & db_periodic_task from huey.contrib.djhuey . This means that these tasks read settings.py for huey configuration (see here ), and as of 2022-07-05, it is not straightforward to override settings in pytest-django . Ideally we should be able to do something like ... import pytest @pytest.fixture def huey_immediate_mode(settings): settings.HUEY[\"immediate\"] = True ... however dropping a breakpoint into a test shows that the huey tasks still think they are in immediate = False mode. This holds even if the tasks module is imported after calling the fixture. So I monkeypatched all tasks huey to force immediate = True ... import huey import pytest @pytest.fixture def huey_immediate_mode(monkeypatch): from stationmanager import tasks _tasks = [ obj for obj in tasks.__dict__.values() if isinstance(obj, huey.api.TaskWrapper) ] for t in _tasks: monkeypatch.setattr(t.huey, \"immediate\", True) ... and success! All tasks now run immediately! django \u00b6 huey \u00b6 pytest \u00b6","title":"How to run `djhuey` tasks in immediate mode"},{"location":"til/override-django-huey-immediate-mode/#how-to-run-djhuey-tasks-in-immediate-mode","text":"I want to test some huey tasks. To do so I need to adapt them into immediate mode. This is complicated by the fact that my tasks are created by decorating functions with db_task & db_periodic_task from huey.contrib.djhuey . This means that these tasks read settings.py for huey configuration (see here ), and as of 2022-07-05, it is not straightforward to override settings in pytest-django . Ideally we should be able to do something like ... import pytest @pytest.fixture def huey_immediate_mode(settings): settings.HUEY[\"immediate\"] = True ... however dropping a breakpoint into a test shows that the huey tasks still think they are in immediate = False mode. This holds even if the tasks module is imported after calling the fixture. So I monkeypatched all tasks huey to force immediate = True ... import huey import pytest @pytest.fixture def huey_immediate_mode(monkeypatch): from stationmanager import tasks _tasks = [ obj for obj in tasks.__dict__.values() if isinstance(obj, huey.api.TaskWrapper) ] for t in _tasks: monkeypatch.setattr(t.huey, \"immediate\", True) ... and success! All tasks now run immediately!","title":"How to run djhuey tasks in immediate mode"},{"location":"til/override-django-huey-immediate-mode/#django","text":"","title":"django"},{"location":"til/override-django-huey-immediate-mode/#huey","text":"","title":"huey"},{"location":"til/override-django-huey-immediate-mode/#pytest","text":"","title":"pytest"},{"location":"til/passing-file-contents-to-package-manager-via-xargs/","text":"I want to install multiple packages specified in dev-requirements.txt via conda jupyter-packaging>=0.10 pytest pytest-cov flake8 nbclassic I can concatenate the file contents to one-line via cat dev-requirements.txt | xargs or jupyter-packaging>=0.10 pytest pytest-cov flake8 nbclassic , and then pass this string to mamba via cat dev-requirements.txt | xargs mamba install -y","title":"Passing file contents to package manager via xargs"},{"location":"til/restore-a-mysql-database-on-docker-compose/","text":"Restore a mysql database on docker compose \u00b6 This one was tough. I had a Docker Compose yml of ... version: \"3.7\" services: db: image: mysql volumes: - ./db:/var/lib/mysql - ./backups/datadump.sql:/docker-entrypoint-initdb.d/datadump.sql environment: - MYSQL_DATABASE=$MYSQL_NAME - MYSQL_USER=$MYSQL_USER - MYSQL_PASSWORD=$MYSQL_PASSWORD - MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD ... web: build: . command: python manage.py runserver 0.0.0.0:8000 volumes: - .:/app ports: - \"8000:8000\" depends_on: - db ... and couldn't work out how to alter the database permissions so that I could access the database from a host other than the db localhost. Specifically, I kept running into user 'username'@'IP' cannot access 'databasename' when launching Docker Compose . From the MySQL Docker docs (and Stackoverflow and Docker Community ...) I found that setting the environment variable MYSQL_ROOT_HOST to % enables accessing the database from a any IP address. After much pain I found out (by reading the Shell script docker-entrypoint.sh for this container) that MYSQL_ROOT_HOST is automatically set to % ! This meant that the datadump.sql file being run to restore the database overwrites user permissions! A quick scan of the sql file validated this! Now how do I overwrite the overwrite!? The bloody MySQL docs have the answer! I just needed to ... ... run mysqld --skip-grant-tables to enable connecting to the server without a password and with all privileges ... ... - MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD command: --skip-grant-tables ... ... launch the database ... docker compose up db ... hook into the running database server ... docker exec -it stationmanager-db-1 mysql ... tell the server to reload the grant tables so that account-management statements work .. FLUSH PRIVILEGES; ... add the user permissions needed to access the database ... ALTER USER 'username'@'%' IDENTIFIED BY 'MyNewPass'; GRANT ALL ON *.* TO 'username'@'%' WITH GRANT OPTION ; ... and finally the Docker web container can now connect to the db server which is running our restored database! docker \u00b6","title":"Restore a mysql database on docker compose"},{"location":"til/restore-a-mysql-database-on-docker-compose/#restore-a-mysql-database-on-docker-compose","text":"This one was tough. I had a Docker Compose yml of ... version: \"3.7\" services: db: image: mysql volumes: - ./db:/var/lib/mysql - ./backups/datadump.sql:/docker-entrypoint-initdb.d/datadump.sql environment: - MYSQL_DATABASE=$MYSQL_NAME - MYSQL_USER=$MYSQL_USER - MYSQL_PASSWORD=$MYSQL_PASSWORD - MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD ... web: build: . command: python manage.py runserver 0.0.0.0:8000 volumes: - .:/app ports: - \"8000:8000\" depends_on: - db ... and couldn't work out how to alter the database permissions so that I could access the database from a host other than the db localhost. Specifically, I kept running into user 'username'@'IP' cannot access 'databasename' when launching Docker Compose . From the MySQL Docker docs (and Stackoverflow and Docker Community ...) I found that setting the environment variable MYSQL_ROOT_HOST to % enables accessing the database from a any IP address. After much pain I found out (by reading the Shell script docker-entrypoint.sh for this container) that MYSQL_ROOT_HOST is automatically set to % ! This meant that the datadump.sql file being run to restore the database overwrites user permissions! A quick scan of the sql file validated this! Now how do I overwrite the overwrite!? The bloody MySQL docs have the answer! I just needed to ... ... run mysqld --skip-grant-tables to enable connecting to the server without a password and with all privileges ... ... - MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD command: --skip-grant-tables ... ... launch the database ... docker compose up db ... hook into the running database server ... docker exec -it stationmanager-db-1 mysql ... tell the server to reload the grant tables so that account-management statements work .. FLUSH PRIVILEGES; ... add the user permissions needed to access the database ... ALTER USER 'username'@'%' IDENTIFIED BY 'MyNewPass'; GRANT ALL ON *.* TO 'username'@'%' WITH GRANT OPTION ; ... and finally the Docker web container can now connect to the db server which is running our restored database!","title":"Restore a mysql database on docker compose"},{"location":"til/restore-a-mysql-database-on-docker-compose/#docker","text":"","title":"docker"},{"location":"til/run-django-via-docker-compose/","text":"How to run django via docker compose \u00b6 I want to run a Django application on docker-compose so that I can define the required infrastructure and dependencies as code and run this application on any machine. The Official Docker Django tutorial demonstrates how to setup a minimal Django application on docker-compose . I want to use a mysql database instead of postgres which means that I need to configure my docker-compose.yml differently. Following the Official MySQL Docker guide I can use ... ... version: '3.7' services: db: image: mysql volumes: - ./data/db:/var/lib/mysql environment: - MYSQL_DATABASE=mysql - MYSQL_USER=mysql - MYSQL_PASSWORD=mysql - MYSQL_RANDOM_ROOT_PASSWORD=1 ports: - \"3406:3306\" web: build: . command: python manage.py runserver 0.0.0.0:8000 ports: - \"8000:8000\" volumes: - .:/app depends_on: - db ... Note: - I had to change the default port on my host ( Windows Subsystem for Linux 2 ) to 3406 as 3306 wasn't available - how-do-i-change-the-default-port-on-which-my-docker-mysql-instance-runs - I tried adding /tmp/app/mysqld:/var/run/mysqld and /tmp/app/mysqld:/run/mysqld as volumes to db and web respectively to map the contents of mysqld to a local folder as suggested by dockerizing-a-django-mysql-project-g4m but ran into permissions issues whereby the web container couldn't write mysqlx.sock.lock to the /run/mysqld ! This file contains socket information that enables the web service to talk to the database service I also need to edit my application's settings.py to point to this docker-compose database ... DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'mysql', 'USER': 'mysql', 'PASSWORD': 'mysql', 'HOST': 'db', 'PORT': 3306, } } Now when I run docker-compose up , both images build fine but MySQL causes problems ... django-container-deployment-mysql-docker-deployment/ suggests installing a Python MySQL connector via pip install mysqlclient in the Django application to enable this connection between db and web which requires system dependencies in the Dockerfile ... ... RUN apt update && apt install -y \\ g++ \\ default-libmysqlclient-dev \\ python3-dev \\ build-essential ... docker \u00b6","title":"How to run django via docker compose"},{"location":"til/run-django-via-docker-compose/#how-to-run-django-via-docker-compose","text":"I want to run a Django application on docker-compose so that I can define the required infrastructure and dependencies as code and run this application on any machine. The Official Docker Django tutorial demonstrates how to setup a minimal Django application on docker-compose . I want to use a mysql database instead of postgres which means that I need to configure my docker-compose.yml differently. Following the Official MySQL Docker guide I can use ... ... version: '3.7' services: db: image: mysql volumes: - ./data/db:/var/lib/mysql environment: - MYSQL_DATABASE=mysql - MYSQL_USER=mysql - MYSQL_PASSWORD=mysql - MYSQL_RANDOM_ROOT_PASSWORD=1 ports: - \"3406:3306\" web: build: . command: python manage.py runserver 0.0.0.0:8000 ports: - \"8000:8000\" volumes: - .:/app depends_on: - db ... Note: - I had to change the default port on my host ( Windows Subsystem for Linux 2 ) to 3406 as 3306 wasn't available - how-do-i-change-the-default-port-on-which-my-docker-mysql-instance-runs - I tried adding /tmp/app/mysqld:/var/run/mysqld and /tmp/app/mysqld:/run/mysqld as volumes to db and web respectively to map the contents of mysqld to a local folder as suggested by dockerizing-a-django-mysql-project-g4m but ran into permissions issues whereby the web container couldn't write mysqlx.sock.lock to the /run/mysqld ! This file contains socket information that enables the web service to talk to the database service I also need to edit my application's settings.py to point to this docker-compose database ... DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'mysql', 'USER': 'mysql', 'PASSWORD': 'mysql', 'HOST': 'db', 'PORT': 3306, } } Now when I run docker-compose up , both images build fine but MySQL causes problems ... django-container-deployment-mysql-docker-deployment/ suggests installing a Python MySQL connector via pip install mysqlclient in the Django application to enable this connection between db and web which requires system dependencies in the Dockerfile ... ... RUN apt update && apt install -y \\ g++ \\ default-libmysqlclient-dev \\ python3-dev \\ build-essential ...","title":"How to run django via docker compose"},{"location":"til/run-django-via-docker-compose/#docker","text":"","title":"docker"},{"location":"til/run-multiple-commands-on-docker-container-startup/","text":"I want to run python manage.py collectstatic on startup. I have a file called startup.sh that includes this command as part of its startup. Trouble is that I often run docker compose run web bash in which case this command is skipped. When you run docker like this: docker run -i -t ubuntu bash the entrypoint is the default /bin/sh -c , the image is ubuntu and the command is bash . If you were executing docker run -i -t ubuntu <cmd> . The parameter of the entrypoint is <cmd> . docker - What is the difference between CMD and ENTRYPOINT in a Dockerfile? - Stack Overflow It makes sense to create a debug.sh command like ... #!/bin/sh python manage.py collectstatic bash ... which will be my command via docker compose run web sh debug.sh docker \u00b6 django \u00b6","title":"Run multiple commands on docker container startup"},{"location":"til/run-multiple-commands-on-docker-container-startup/#docker","text":"","title":"docker"},{"location":"til/run-multiple-commands-on-docker-container-startup/#django","text":"","title":"django"},{"location":"til/running-scripts-on-launch/","text":"Running scripts on launch \u00b6 I wanted to set some aliases for conda in a separate conda-aliases.ps1 script and activate custom oh-my-posh configuration rdmolony.omp.json on startup both of which are saved in the same directory as $PROFILE so I can track them in the same repository. Running . .\\conda-aliases.ps1 works fine when opening in a shell in the $PROFILE directory, however, when changing directory powershell can no longer find the file! I need to pass an absolute path to the script instead, however, I need to make use of an environmental variable holding the $PROFILE directory to make this generalisable. I can use either (Get-ChildItem $PROFILE) (see here ) or for the parent of any file $PSScriptRoot) ( here ) :book: I can use dir env: to get a list of all environmental variables Replacing . \".\\conda-aliases.ps1\" With . (Join-Path $PSScriptRoot \".\\conda-aliases.ps1\") Does the trick ( here ! :exclamation: See profile here powershell \u00b6","title":"Running scripts on launch"},{"location":"til/running-scripts-on-launch/#running-scripts-on-launch","text":"I wanted to set some aliases for conda in a separate conda-aliases.ps1 script and activate custom oh-my-posh configuration rdmolony.omp.json on startup both of which are saved in the same directory as $PROFILE so I can track them in the same repository. Running . .\\conda-aliases.ps1 works fine when opening in a shell in the $PROFILE directory, however, when changing directory powershell can no longer find the file! I need to pass an absolute path to the script instead, however, I need to make use of an environmental variable holding the $PROFILE directory to make this generalisable. I can use either (Get-ChildItem $PROFILE) (see here ) or for the parent of any file $PSScriptRoot) ( here ) :book: I can use dir env: to get a list of all environmental variables Replacing . \".\\conda-aliases.ps1\" With . (Join-Path $PSScriptRoot \".\\conda-aliases.ps1\") Does the trick ( here ! :exclamation: See profile here","title":"Running scripts on launch"},{"location":"til/running-scripts-on-launch/#powershell","text":"","title":"powershell"},{"location":"til/save-files-via-django-modelform/","text":"I wanted to save a user uploaded file to disk on our production server. The simplest way seems to be to create a model & to link a form to this model via ModelForm which fills a form with predefined model fields. I can render my model WindogFlag ... class WindogFlag(models.Model): flags = models.FileField(upload_to=\"flags/\") upload_date = models.DateField(auto_now=True) ... as a form WindogFlagForm ... class WindogFlagForm(forms.ModelForm): class Meta: model = models.WindogFlag fields = ['flags'] ... and render it in myview ... def myview(request): ... if request.method == 'POST': form = WindogFlagForm(request.POST, request.FILES) file = request.FILES[\"flags\"] if form.is_valid(): try: upload_windog_flags_to_datalog(file, station, request) except Exception as e: msg = f\"Upload Windographer Flags Failed: {e.__class__} {e}\" messages.error(request, msg) else: form.save() return redirect(request.path) else: raise ValueError(form.errors) else: form = WindogFlagForm() ... The file won't be saved to disk until form.save() is called! I can test then this with pytest ... @pytest.mark.django_db @pytest.mark.usefixtures(\"ch020_01_factory\") @pytest.mark.usefixtures(\"fill_flags\") def test_windog_flags_are_saved_to_disk( admin_client, ch020_01_windog_flags_txt: Path, tmp_path: Path, monkeypatch ) -> None: client = admin_client flags_file = ch020_01_windog_flags_txt filename = flags_file.name station = models.Station.objects.get(name=\"ch020_01\") monkeypatch.setattr( \"stationmanager.models.WindogFlag.flags.field.storage.location\", tmp_path ) with open(flags_file, \"r\") as fp: client.post(f\"/stationmanager/station/{station.id}\", {\"flags\": fp}) uploaded_flags = [f.name for f in tmp_path.iterdir()] assert filename in uploaded_flags Annoyingly it seems that the pytest-django fixture settings doesn't work when changing MEDIA_ROOT as Django saves MEDIA_ROOT in WindogFlag.flags.field.storage.location on intialisation of the WindogFlag model","title":"Save files via django modelform"},{"location":"til/select-all-text-between-two-strings-across-lines-via-regex/","text":"Select all text between two strings across lines via regex \u00b6 I want to remove a block of text from the docstrings of 20+ functions at once where each block of text coincides with .. plot:: and \"\"\" ... ... .. plot:: :include-source: import mrp from mrp import crosstab test_station = mrp.load_test_station(mrp.__file__) ct = crosstab.heightbydata( test_station, test_station.t1avg, 2 ) crosstab.plot(ct, show_values=False) \"\"\" I can't use the Meta Escape character . as this does not include line terminators \\n . I can, however, use [\\w\\W] , which means select all words and non-words, alongside the one or more lazy quantifier +? where ? forces it to match as few words as possible ... (.. plot::[\\w\\W]+?)\"\"\" regex \u00b6","title":"Select all text between two strings across lines via regex"},{"location":"til/select-all-text-between-two-strings-across-lines-via-regex/#select-all-text-between-two-strings-across-lines-via-regex","text":"I want to remove a block of text from the docstrings of 20+ functions at once where each block of text coincides with .. plot:: and \"\"\" ... ... .. plot:: :include-source: import mrp from mrp import crosstab test_station = mrp.load_test_station(mrp.__file__) ct = crosstab.heightbydata( test_station, test_station.t1avg, 2 ) crosstab.plot(ct, show_values=False) \"\"\" I can't use the Meta Escape character . as this does not include line terminators \\n . I can, however, use [\\w\\W] , which means select all words and non-words, alongside the one or more lazy quantifier +? where ? forces it to match as few words as possible ... (.. plot::[\\w\\W]+?)\"\"\"","title":"Select all text between two strings across lines via regex"},{"location":"til/select-all-text-between-two-strings-across-lines-via-regex/#regex","text":"","title":"regex"},{"location":"til/speed-up-table-load-via-prefetch/","text":"I have a table defined in Django . It's takes ~1m to load due to ~2000 queries being required. Each row represents a measurement station, and each station links to numerous sensors via a ManyToManyColumn . By default, django-tables2 a separate query for every station to evaluate the sensors. Each sensor links to a node representing a discrete height on the measurement station. django-tables2 again makes a separate query for every node for every sensor! We can speed this up using prefetch_related & select_related Initially the view looks like ... def _colour_sensor_on_quality(quality: int) -> str: if quality < 50: colour = \"red\" elif quality < 75: colour = \"orange\" else: colour = \"green\" return colour class StationTable(tables.Table): class Meta: model = Station template_name = \"django_tables2/bootstrap.html\" fields = (\"name\", \"sensorquality_set\") sensorquality_set = tables.ManyToManyColumn( verbose_name=\"Sensors\", transform=( lambda s: format_html( '<span' + ( f' style=\"color: {_colour_sensor_on_quality(s.last_week)}\"' if s.last_week is not None else '' ) + ' data-toggle=\"tooltip\"' + ' data-placement=\"top\"' + ' data-html=\"true\"' + ' title=\"' + f'Overall: {s.overall}\\n' + f'Last month: {s.last_month}\\n' + f'Last week: {s.last_week}\\n' + f'Most recent timestamp: {s.most_recent_timestamp}\\n' + f'Quality check last run: {s.last_modified}' + '\"' +'>' + str(s.node) + '</span>' ) ), ) class StationListView(LoginRequiredMixin, SingleTableMixin): model = models.Station table_class = tables.StationTable template_name = 'stationmanager/station_list.html' Overriding the table queryset to use fetch_related on node_set & sensor_quality_set reduces page loadiing to ~2000 queries in 2.9s ... class StationListView(LoginRequiredMixin, SingleTableMixin, FilterView): model = models.Station table_class = tables.StationTable template_name = 'stationmanager/station_list.html' def get_queryset(self): qs = super().get_queryset() qs = qs.prefetch_related(\"sensorquality_set\", \"node_set\") return qs ... which generates SQL queries like ... ** SELECT** `stationmanager_sensorquality`.`id`, `stationmanager_sensorquality`.`station_id`, `stationmanager_sensorquality`.`sensor`, `stationmanager_sensorquality`.`node_id`, `stationmanager_sensorquality`.`overall`, `stationmanager_sensorquality`.`last_month`, `stationmanager_sensorquality`.`last_week`, `stationmanager_sensorquality`.`most_recent_timestamp`, `stationmanager_sensorquality`.`last_modified` **FROM** `stationmanager_sensorquality` **WHERE** `stationmanager_sensorquality`.`station_id` IN (1470, 230, 592, 610, 611, 612, 553, 1425, 2098, 2102, 2103, 2190, 474, 475, 109, 110, 1480, 17, 129, 130, 131, 237, 507, 542, 1436) Overriding the queryset to use fetch_related for sensorquality_set and a select_related on node_set relative to sensor_quality_set is even faster; 5 queries in 1s! select_related joins the sensorquality table with the related foreign key nodes before prefetching via ... class StationListView(LoginRequiredMixin, SingleTableMixin, FilterView): model = models.Station table_class = tables.StationTable template_name = 'stationmanager/station_list.html' def get_queryset(self): qs = super().get_queryset() # NOTE: Join nodes & sensorquality into a single table via select_related # ... then prefetch the sensorquality related to the stations in the current # ... table view # ... Otherwise tables2 makes separate queries for sensorqualities & then nodes # ... so would run ~100x slower # https://stackoverflow.com/questions/54569384/django-chaining-prefetch-related-and-select-related sensorquality_set = Prefetch( \"sensorquality_set\", queryset=models.SensorQuality.objects.select_related(\"node\") ) qs = qs.prefetch_related(sensorquality_set) return qs ... which generates ... **SELECT** `stationmanager_sensorquality`.`id`, `stationmanager_sensorquality`.`station_id`, `stationmanager_sensorquality`.`sensor`, `stationmanager_sensorquality`.`node_id`, `stationmanager_sensorquality`.`overall`, `stationmanager_sensorquality`.`last_month`, `stationmanager_sensorquality`.`last_week`, `stationmanager_sensorquality`.`most_recent_timestamp`, `stationmanager_sensorquality`.`last_modified`, `node`.`id`, `node`.`idStation`, `node`.`idDataType`, `node`.`Height`, `node`.`Magnetic_Orientation`, `node`.`Boom_Length`, `node`.`Inclination_Angle`, `node`.`is_IEC_Compliant`, `node`.`Raw_DB_Field`, `node`.`Raw_DB_Field_Std`, `node`.`Raw_DB_Field_Max`, `node`.`Raw_DB_Field_Min`, `node`.`raw_db_field_gust_max` **FROM** `stationmanager_sensorquality` **INNER JOIN** `node` **ON** (`stationmanager_sensorquality`.`node_id` = `node`.`id`) **WHERE** `stationmanager_sensorquality`.`station_id` IN (1470, 230, 592, 610, 611, 612, 553, 1425, 2098, 2102, 2103, 2190, 474, 475, 109, 110, 1480, 17, 129, 130, 131, 237, 507, 542, 1436) Inspired by ... Django for Professionals by Will Vincent & https://stackoverflow.com/questions/54569384/django-chaining-prefetch-related-and-select-related django \u00b6 python \u00b6 django-tables2 \u00b6","title":"Speed up table load via prefetch"},{"location":"til/speed-up-table-load-via-prefetch/#django","text":"","title":"django"},{"location":"til/speed-up-table-load-via-prefetch/#python","text":"","title":"python"},{"location":"til/speed-up-table-load-via-prefetch/#django-tables2","text":"","title":"django-tables2"},{"location":"til/styling-powershell-with-oh-my-posh/","text":"Styling Powershell with oh-my-posh \u00b6 Coming from oh-my-zsh I wanted to style my prompt to a fancy theme and enable answering the following questions with my shell prompt: What git branch am I on? Am I up to date with upstream? Is my virtual environment activated? The only catch was installing a font which can display all of the emojis used by each theme. nerd-fonts Meslo is recommended by oh-my-posh , however, even the mono Meslo fonts have too much whitespace between characters and look weird in vscode . The powerlevel10k MesloLGS NF Regular.ttf does work. After installing my font I checked it was installed by going to C:\\Windows\\Fonts and finally I copied and pasted the filename into Settings > Terminal > Font Family to set it as the default in vscode :exclamation: See profile here powershell \u00b6","title":"Styling Powershell with [`oh-my-posh`](https://github.com/jandedobbeleer/oh-my-posh)"},{"location":"til/styling-powershell-with-oh-my-posh/#styling-powershell-with-oh-my-posh","text":"Coming from oh-my-zsh I wanted to style my prompt to a fancy theme and enable answering the following questions with my shell prompt: What git branch am I on? Am I up to date with upstream? Is my virtual environment activated? The only catch was installing a font which can display all of the emojis used by each theme. nerd-fonts Meslo is recommended by oh-my-posh , however, even the mono Meslo fonts have too much whitespace between characters and look weird in vscode . The powerlevel10k MesloLGS NF Regular.ttf does work. After installing my font I checked it was installed by going to C:\\Windows\\Fonts and finally I copied and pasted the filename into Settings > Terminal > Font Family to set it as the default in vscode :exclamation: See profile here","title":"Styling Powershell with oh-my-posh"},{"location":"til/styling-powershell-with-oh-my-posh/#powershell","text":"","title":"powershell"},{"location":"til/switch-selenium-user-within-a-test/","text":"Switch selenium user within a test \u00b6 I want to set up and tear down a Selenium webdriver twice in the same test so that I can test how my Django app behaves for multiple users. By default Selenium hangs if the test function test_multiple_users_can_start_lists_at_different_urls fails at any point beyond quitting and restarting the browser ... import re import socket import time import pytest from pytest_django.live_server_helper import LiveServer from selenium import webdriver from selenium.webdriver.common.keys import Keys from selenium.webdriver.common.desired_capabilities import DesiredCapabilities def _get_remote_webdriver() -> webdriver.Remote: return webdriver.Remote( command_executor=\"http://selenium:4444/wd/hub\", desired_capabilities=DesiredCapabilities.CHROME, ) @pytest.fixture def webdriver_init() -> webdriver.Remote: browser = _get_remote_webdriver() yield browser browser.quit() def _get_web_container_ipaddess() -> str: host_name = socket.gethostname() host_ipaddress = socket.gethostbyname(host_name) return host_ipaddress @pytest.fixture def live_server_at_web_container_ipaddress() -> LiveServer: # Set host to externally accessible web server address web_container_ip_address = _get_web_container_ipaddess() return LiveServer(addr=web_container_ip_address) @pytest.mark.django_db def test_multiple_users_can_start_lists_at_different_urls( webdriver_init: webdriver.Remote, live_server_at_web_container_ipaddress: LiveServer, ) -> None: browser = webdriver_init live_server_url = str(live_server_at_web_container_ipaddress) # Edith starts a new to-do list browser.get(live_server_url) inputbox = browser.find_element_by_id(\"id_new_item\") inputbox.send_keys(\"Buy peacock feathers\") inputbox.send_keys(Keys.ENTER) wait_for_row_in_list_table(browser, \"1: Buy peacock feathers\") # She notices that her list has a unique URL edith_list_url = browser.current_url assert re.search( \"/lists/.+\", edith_list_url ), f\"Regex didn't match: 'lists/.+' not found in {edith_list_url}\" # Now a new user, Francis, comes along to the site. ## We use a new browser session to make sure that no information ## of Edith's is coming through from cookies etc browser.quit() browser = _get_remote_webdriver() # Francis visits the home page. There is no sign of Edith's # list browser.get(live_server_url) page_text = browser.find_element_by_tag_name(\"body\").text assert \"Buy peacock feathers\" not in page_text assert \"make a fly\" not in page_text # Francis starts a list by entering a new item. He # is less interesting than Edith... inputbox = browser.find_element_by_id(\"id_new_item\") inputbox.send_keys(\"Buy milk\") inputbox.send_keys(Keys.ENTER) wait_for_row_in_list_table(browser, \"1: Buy milk\") # Francis gets his own unique URL francis_list_url = browser.current_url assert re.search( \"/lists/.+\", francis_list_url ), f\"Regex didn't match: 'lists/.+' not found in {francis_list_url}\" assert francis_list_url != edith_list_url # Again, there is no trace of Edith's list page_text = browser.find_element_by_tag_name(\"body\").text assert \"Buy peacock feathers\" not in page_text assert \"Buy milk\" in page_text # Satisfied, they both go back to sleep I added a breakpoint after yield in webdriver_init ... > /app/functional_tests/tests.py(34)webdriver_init() -> browser.quit() (Pdb) browser.quit() *** selenium.common.exceptions.WebDriverException: Message: Unable to execute request for an existing session: Unable to find session with ID: 0b215f3be5545e376c41ba5d1695e6d6 Build info: version: '4.1.0', revision: '87802e897b' System info: host: 'f9d7fe69645a', ip: '172.18.0.2', os.name: 'Linux', os.arch: 'amd64', os.version: '5.10.60.1-microsoft-standard-WSL2', java.version: '11.0.11' Driver info: driver.version: unknown ... which seems to fail because the 2nd browser session is created with a different session_id to the 1st browser session and so Selenium cannot quit a browser session which is already over. I'm hacking around this by replacing ... ... browser.quit() browser = _get_remote_webdriver() ... ... with ... ... browser.delete_all_cookies() ... pytest \u00b6","title":"Switch selenium user within a test"},{"location":"til/switch-selenium-user-within-a-test/#switch-selenium-user-within-a-test","text":"I want to set up and tear down a Selenium webdriver twice in the same test so that I can test how my Django app behaves for multiple users. By default Selenium hangs if the test function test_multiple_users_can_start_lists_at_different_urls fails at any point beyond quitting and restarting the browser ... import re import socket import time import pytest from pytest_django.live_server_helper import LiveServer from selenium import webdriver from selenium.webdriver.common.keys import Keys from selenium.webdriver.common.desired_capabilities import DesiredCapabilities def _get_remote_webdriver() -> webdriver.Remote: return webdriver.Remote( command_executor=\"http://selenium:4444/wd/hub\", desired_capabilities=DesiredCapabilities.CHROME, ) @pytest.fixture def webdriver_init() -> webdriver.Remote: browser = _get_remote_webdriver() yield browser browser.quit() def _get_web_container_ipaddess() -> str: host_name = socket.gethostname() host_ipaddress = socket.gethostbyname(host_name) return host_ipaddress @pytest.fixture def live_server_at_web_container_ipaddress() -> LiveServer: # Set host to externally accessible web server address web_container_ip_address = _get_web_container_ipaddess() return LiveServer(addr=web_container_ip_address) @pytest.mark.django_db def test_multiple_users_can_start_lists_at_different_urls( webdriver_init: webdriver.Remote, live_server_at_web_container_ipaddress: LiveServer, ) -> None: browser = webdriver_init live_server_url = str(live_server_at_web_container_ipaddress) # Edith starts a new to-do list browser.get(live_server_url) inputbox = browser.find_element_by_id(\"id_new_item\") inputbox.send_keys(\"Buy peacock feathers\") inputbox.send_keys(Keys.ENTER) wait_for_row_in_list_table(browser, \"1: Buy peacock feathers\") # She notices that her list has a unique URL edith_list_url = browser.current_url assert re.search( \"/lists/.+\", edith_list_url ), f\"Regex didn't match: 'lists/.+' not found in {edith_list_url}\" # Now a new user, Francis, comes along to the site. ## We use a new browser session to make sure that no information ## of Edith's is coming through from cookies etc browser.quit() browser = _get_remote_webdriver() # Francis visits the home page. There is no sign of Edith's # list browser.get(live_server_url) page_text = browser.find_element_by_tag_name(\"body\").text assert \"Buy peacock feathers\" not in page_text assert \"make a fly\" not in page_text # Francis starts a list by entering a new item. He # is less interesting than Edith... inputbox = browser.find_element_by_id(\"id_new_item\") inputbox.send_keys(\"Buy milk\") inputbox.send_keys(Keys.ENTER) wait_for_row_in_list_table(browser, \"1: Buy milk\") # Francis gets his own unique URL francis_list_url = browser.current_url assert re.search( \"/lists/.+\", francis_list_url ), f\"Regex didn't match: 'lists/.+' not found in {francis_list_url}\" assert francis_list_url != edith_list_url # Again, there is no trace of Edith's list page_text = browser.find_element_by_tag_name(\"body\").text assert \"Buy peacock feathers\" not in page_text assert \"Buy milk\" in page_text # Satisfied, they both go back to sleep I added a breakpoint after yield in webdriver_init ... > /app/functional_tests/tests.py(34)webdriver_init() -> browser.quit() (Pdb) browser.quit() *** selenium.common.exceptions.WebDriverException: Message: Unable to execute request for an existing session: Unable to find session with ID: 0b215f3be5545e376c41ba5d1695e6d6 Build info: version: '4.1.0', revision: '87802e897b' System info: host: 'f9d7fe69645a', ip: '172.18.0.2', os.name: 'Linux', os.arch: 'amd64', os.version: '5.10.60.1-microsoft-standard-WSL2', java.version: '11.0.11' Driver info: driver.version: unknown ... which seems to fail because the 2nd browser session is created with a different session_id to the 1st browser session and so Selenium cannot quit a browser session which is already over. I'm hacking around this by replacing ... ... browser.quit() browser = _get_remote_webdriver() ... ... with ... ... browser.delete_all_cookies() ...","title":"Switch selenium user within a test"},{"location":"til/switch-selenium-user-within-a-test/#pytest","text":"","title":"pytest"},{"location":"til/test-a-django-app-on-docker-via-selenium-2/","text":"Test a django app on docker via selenium 2 \u00b6 Marc Gibbons blog provides a nice working example but it doesn't explain how it works in as much detail as I'd like. Marc uses Docker links to enable referring to the selenium container using its name rather than its IP address ... version: '3' services: django: build: . volumes: - \".:/code\" links: - selenium ports: - \"8000:8000\" selenium: image: selenium/standalone-chrome-debug:3.7.1 ports: - 4444:4444 - 5900:5900 ... which enables hooking into the Selenium RemoteDriver running elsewhere ... from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities browser = webdriver.Remote( command_executor='http://selenium:4444/wd/hub', desired_capabilities=DesiredCapabilities.CHROME, ) For the following code ... @tag('selenium') @override_settings(ALLOWED_HOSTS=['*']) class BaseTestCase(StaticLiveServerTestCase): \"\"\" Provides base test class which connects to the Docker container running selenium. \"\"\" host = '0.0.0.0' @classmethod def setUpClass(cls): super().setUpClass() cls.host = socket.gethostbyname(socket.gethostname()) cls.selenium = webdriver.Remote( command_executor='http://selenium:4444/wd/hub', desired_capabilities=DesiredCapabilities.CHROME, ) cls.selenium.implicitly_wait(5) ... host defines running a StaticLiveServerTestCase similar to python manage.py runserver 0.0.0.0:8000 , and setting cls.host to socket.gethostbyname(socket.gethostname()) only impacts self.live_server_url by replacing 0.0.0.0 with the docker container IP address. This enables accessing the running web container from the selenium container. I adapt this example to run on pytest instead of unittest via pytest-django at rdmolony/django-selenium-pytest-docker I struggled to adapt Harry Percival's obeythetestinggoat examples initial functional test example. In this case I want to run the Selenium RemoteDriver on a running web server (started via python manage.py runserver 0.0.0.0:8000 ). To access this running server from the Selenium browser I created http://{host}/8000 where host is socket.gethostbyname(socket.gethostname()) . host was 172.22.0.4 instead of 172.22.0.3 . It turns out that I was creating multiple shells in the web container incorrectly. I launched web via ... docker compose run --name goat-web --rm web python manage.py runserver 0.0.0.0:8000 ... and I hooked into this running server to run my functional tests via ... docker compose run --rm web python functional_tests.py ... which is wrong! Hooking into web creates a separate container with a separate IP address. If instead I hook into the running server via exec instead of run it works fine ... docker exec -it goat-web python functional_tests.py I got sidetracked reading into Docker networking. It turns out that Docker Compose creates a network on docker compose up which means that web can access selenium via http://selenium instead of running its IP address and so does not need to be explicitely linked via links ! docker \u00b6","title":"Test a django app on docker via selenium 2"},{"location":"til/test-a-django-app-on-docker-via-selenium-2/#test-a-django-app-on-docker-via-selenium-2","text":"Marc Gibbons blog provides a nice working example but it doesn't explain how it works in as much detail as I'd like. Marc uses Docker links to enable referring to the selenium container using its name rather than its IP address ... version: '3' services: django: build: . volumes: - \".:/code\" links: - selenium ports: - \"8000:8000\" selenium: image: selenium/standalone-chrome-debug:3.7.1 ports: - 4444:4444 - 5900:5900 ... which enables hooking into the Selenium RemoteDriver running elsewhere ... from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities browser = webdriver.Remote( command_executor='http://selenium:4444/wd/hub', desired_capabilities=DesiredCapabilities.CHROME, ) For the following code ... @tag('selenium') @override_settings(ALLOWED_HOSTS=['*']) class BaseTestCase(StaticLiveServerTestCase): \"\"\" Provides base test class which connects to the Docker container running selenium. \"\"\" host = '0.0.0.0' @classmethod def setUpClass(cls): super().setUpClass() cls.host = socket.gethostbyname(socket.gethostname()) cls.selenium = webdriver.Remote( command_executor='http://selenium:4444/wd/hub', desired_capabilities=DesiredCapabilities.CHROME, ) cls.selenium.implicitly_wait(5) ... host defines running a StaticLiveServerTestCase similar to python manage.py runserver 0.0.0.0:8000 , and setting cls.host to socket.gethostbyname(socket.gethostname()) only impacts self.live_server_url by replacing 0.0.0.0 with the docker container IP address. This enables accessing the running web container from the selenium container. I adapt this example to run on pytest instead of unittest via pytest-django at rdmolony/django-selenium-pytest-docker I struggled to adapt Harry Percival's obeythetestinggoat examples initial functional test example. In this case I want to run the Selenium RemoteDriver on a running web server (started via python manage.py runserver 0.0.0.0:8000 ). To access this running server from the Selenium browser I created http://{host}/8000 where host is socket.gethostbyname(socket.gethostname()) . host was 172.22.0.4 instead of 172.22.0.3 . It turns out that I was creating multiple shells in the web container incorrectly. I launched web via ... docker compose run --name goat-web --rm web python manage.py runserver 0.0.0.0:8000 ... and I hooked into this running server to run my functional tests via ... docker compose run --rm web python functional_tests.py ... which is wrong! Hooking into web creates a separate container with a separate IP address. If instead I hook into the running server via exec instead of run it works fine ... docker exec -it goat-web python functional_tests.py I got sidetracked reading into Docker networking. It turns out that Docker Compose creates a network on docker compose up which means that web can access selenium via http://selenium instead of running its IP address and so does not need to be explicitely linked via links !","title":"Test a django app on docker via selenium 2"},{"location":"til/test-a-django-app-on-docker-via-selenium-2/#docker","text":"","title":"docker"},{"location":"til/test-a-django-app-on-docker-via-selenium/","text":"Test a django app on docker via selenium \u00b6 Marc Gibbons guide Selenium Tests in Django & Docker and accompanying code works like a charm on Windows Subsystems for Linux 2 (WSL2) except for the VNC Viewer which can't be installed on my work laptop without authorisation by IS. VNC means virtual network computing or remotely controlling a computer from anoter device The key is Docker Compose links ... version: '3' services: django: build: . volumes: - \".:/code\" links: - selenium ports: - \"8000:8000\" selenium: image: selenium/standalone-chrome-debug:3.7.1 ports: - 4444:4444 # Selenium - 5900:5900 # VNC Server ... which links the django container to the selenium container which means that containers for the linked service are accessible at a hostname identical to the alias. NoVNC runs on WSL2 without requiring admin priviledges. It enables hooking into the Selenium VNC server. First, I got Selenium running ... docker compose start selenium ... which starts a VNC Server at localhost:5900 which I can hook into with NoVNC by running the novnc_proxy Shell script ... git clone https://github.com/novnc/noVNC cd noVNC ./utils/novnc_proxy --vnc localhost:5900 ... and access in my browser via localhost:5900/vnc.html . I can now run my Django app unit tests and view their execution via NoVNC via ... docker compose run django python manage.py test docker \u00b6","title":"Test a django app on docker via selenium"},{"location":"til/test-a-django-app-on-docker-via-selenium/#test-a-django-app-on-docker-via-selenium","text":"Marc Gibbons guide Selenium Tests in Django & Docker and accompanying code works like a charm on Windows Subsystems for Linux 2 (WSL2) except for the VNC Viewer which can't be installed on my work laptop without authorisation by IS. VNC means virtual network computing or remotely controlling a computer from anoter device The key is Docker Compose links ... version: '3' services: django: build: . volumes: - \".:/code\" links: - selenium ports: - \"8000:8000\" selenium: image: selenium/standalone-chrome-debug:3.7.1 ports: - 4444:4444 # Selenium - 5900:5900 # VNC Server ... which links the django container to the selenium container which means that containers for the linked service are accessible at a hostname identical to the alias. NoVNC runs on WSL2 without requiring admin priviledges. It enables hooking into the Selenium VNC server. First, I got Selenium running ... docker compose start selenium ... which starts a VNC Server at localhost:5900 which I can hook into with NoVNC by running the novnc_proxy Shell script ... git clone https://github.com/novnc/noVNC cd noVNC ./utils/novnc_proxy --vnc localhost:5900 ... and access in my browser via localhost:5900/vnc.html . I can now run my Django app unit tests and view their execution via NoVNC via ... docker compose run django python manage.py test","title":"Test a django app on docker via selenium"},{"location":"til/test-a-django-app-on-docker-via-selenium/#docker","text":"","title":"docker"},{"location":"til/test-huey-tasks/","text":"Test huey tasks \u00b6 I want to unit test some huey tasks that are periodically run by Django to import csv files to a MSSQL database. To do so I must run them in immediate mode , however, this is set in settings.py which is tricky to override in pytest-django . pytest-django implements a pytest fixture called settings that enables overriding settings.py on a test-by-test basis. So I tried settings.HUEY['immediate'] = True prior to importing the huey task. Checking the huey task via import_datafile.huey.immediate shows this doesn't work. Instead, I can override the task manually via import_datafile.huey.immediate = True prior to calling it. This does the trick django \u00b6","title":"Test huey tasks"},{"location":"til/test-huey-tasks/#test-huey-tasks","text":"I want to unit test some huey tasks that are periodically run by Django to import csv files to a MSSQL database. To do so I must run them in immediate mode , however, this is set in settings.py which is tricky to override in pytest-django . pytest-django implements a pytest fixture called settings that enables overriding settings.py on a test-by-test basis. So I tried settings.HUEY['immediate'] = True prior to importing the huey task. Checking the huey task via import_datafile.huey.immediate shows this doesn't work. Instead, I can override the task manually via import_datafile.huey.immediate = True prior to calling it. This does the trick","title":"Test huey tasks"},{"location":"til/test-huey-tasks/#django","text":"","title":"django"},{"location":"til/update-git-submodules/","text":"I can recursively update submodules via git submodule foreach git pull https://stackoverflow.com/questions/33714063/how-to-update-submodules-in-git git \u00b6","title":"Update git submodules"},{"location":"til/update-git-submodules/#git","text":"","title":"git"},{"location":"til/update-parent-repo-on-submodule-update-via-github-actions/","text":"I want to update my blog rdmolony.github.io on updating til . I can add an action to the til repo to ... Checkout the parent repo or rdmolony/rdmolony.github.io Fetch the latest til Commit the latest SHA ... via ... name: ci on: push: branches: - main jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 with: repository: 'rdmolony/rdmolony.github.io' submodules: true - name: Setup git config run: | git config --global user.name \"GitHub Actions Bot\" git config --global user.email \"<>\" - run: git submodule foreach --recursive git pull - run: git add docs/til && git commit -m \"Update til\" https://stackoverflow.com/questions/62960533/how-to-use-git-commands-during-a-github-action github-actions \u00b6 git \u00b6","title":"Update parent repo on submodule update via github actions"},{"location":"til/update-parent-repo-on-submodule-update-via-github-actions/#github-actions","text":"","title":"github-actions"},{"location":"til/update-parent-repo-on-submodule-update-via-github-actions/#git","text":"","title":"git"}]}